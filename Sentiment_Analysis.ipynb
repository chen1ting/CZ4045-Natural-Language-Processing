{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import regex as re\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import time\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "# preprocessing \n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from langdetect import detect\n",
    "from langdetect import DetectorFactory\n",
    "\n",
    "# word cloud\n",
    "from PIL import Image\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')    # to suppress future warnings etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"0\"></a>\n",
    "#### Contents:\n",
    "* [1. Dataset](#1)\n",
    "    * [1.1. Loading Data](#1.1)\n",
    "    * [1.2. Data Preprocessing](#1.2)\n",
    "    * [1.3. Dataset Visualization](#1.3)\n",
    "* [2. Lexicon based models](#2)\n",
    "    * [2.1. VADER !!TODO:DELETE BEFORE SUBMISSION](#2.1)\n",
    "    * [2.2. Model](#2.2)\n",
    "    * [2.3. Evaluation](#2.3)\n",
    "* [3. TextBlob Model](#3)\n",
    "* [Advanced. Emotions in data](#advanced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"1\"></a>\n",
    "[<font size=\"+2\" ><b>1. Dataset Presentation</b></font><br>](#0)\n",
    "\n",
    "It is wise to explore our dataset first"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"1.1\"></a>\n",
    "[<font size=\"+1\" ><b>1.1. Loading Data</b></font><br>](#0)\n",
    "Load necessary data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing unused columns\n",
    "df_outside = pd.read_csv('dataset_transformed.csv', encoding='latin')\n",
    "df_crawl = pd.read_csv('compiled_tweets.csv')\n",
    "df_outside = df_outside[[\"Text\", \"Subjectivity\", \"Polarity\"]]\n",
    "df_crawl = df_crawl[[\"Text\", \"Subjectivity\", \"Polarity\"]]\n",
    "df_crawl = df_crawl[df_crawl['Subjectivity'].notna()].copy()\n",
    "df = pd.concat([df_crawl, df_outside]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"1.2\"></a>\n",
    "[<font size=\"+1\" ><b>1.2. Data Preprocessing</b></font><br>](#0)\n",
    "\n",
    "As twitter texts are relatively unclean compared with others, special cleaning are applied to remove emojis and links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# set seed\n",
    "DetectorFactory.seed = 0\n",
    "\n",
    "def language_detection(x):\n",
    "    text = x['body_text'].split(\" \")\n",
    "    \n",
    "    lang = \"en\"\n",
    "    try:\n",
    "        if len(text) > 50:\n",
    "            lang = detect(\" \".join(text[:50]))\n",
    "        elif len(text) > 0:\n",
    "            lang = detect(\" \".join(text[:len(text)]))\n",
    "    except Exception as e:\n",
    "        all_words = set(text)\n",
    "        try:\n",
    "            lang = detect(\" \".join(all_words))\n",
    "        except Exception as e:\n",
    "            try:\n",
    "                # let's try to label it through the abstract then\n",
    "                lang = detect(x['abstract'])\n",
    "            except Exception as e:\n",
    "                lang = \"unknown\"\n",
    "                pass\n",
    "    return lang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = str(text)\n",
    "    text = re.sub(r'[^a-zA-Z ]+', ' ', text)\n",
    "    text = re.sub(r'http\\S+', ' ', text)\n",
    "    text = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', text)\n",
    "    text = re.sub(r'^RT[\\s]+', '', text)\n",
    "    # text = re.sub(r'pic.twitter\\S+', ' ', text)\n",
    "    text = re.sub(r'#', '', text)\n",
    "    text = text.lower()\n",
    "\n",
    "    return text\n",
    "\n",
    "def decontracted(text):\n",
    "    text = re.sub(r\"won\\'t\", \"will not\", text)\n",
    "    text = re.sub(r\"don't\", \"do not\", text)\n",
    "    text = re.sub(r\"don't\", \"do not\", text)\n",
    "    text = re.sub(r\"can\\'t\", \"can not\", text)\n",
    "    text = re.sub(r\"n\\'t\", \" not\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"it\\'s\", \"it is\", text)\n",
    "    text = re.sub(r\"\\'d\", \" would\", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
    "    text = re.sub(r\"\\'t\", \" not\", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
    "    text = re.sub(r\"\\'m\", \" am\", text)\n",
    "\n",
    "    text = re.sub(r\"n\\'t\", \" not\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"\\'d\", \" would\", text)\n",
    "    text = re.sub(r\"\\'d\", \" would\", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
    "    text = re.sub(r\"\\'t\", \" not\", text)\n",
    "    text = re.sub(r\"\\'t\", \" not\", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
    "    text = re.sub(r\"\\'m\", \" am\", text)\n",
    "    text = re.sub(r\"\\'m\", \" am\", text)\n",
    "    text = re.sub(r\"\\“\", \"\", text)\n",
    "    text = re.sub(r\"\\”\", \"\", text)\n",
    "    text = re.sub(r\"\\…\", \"\", text)\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_punc(tweet):\n",
    "    tweet =  tweet.translate(str.maketrans('', '', string.punctuation))\n",
    "    tweet = ' '.join([word for word in tweet.split()])\n",
    "    tweet = tweet.lower()\n",
    "    \n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Text'] = df['Text'].progress_apply(clean_text).apply(decontracted).apply(remove_punc)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "def stem_text(x):\n",
    "    stemmer = PorterStemmer()\n",
    "    x = word_tokenize(x)\n",
    "    stem = ''\n",
    "    \n",
    "    for i in x:\n",
    "        stem += stemmer.stem(i) + ' '\n",
    "        \n",
    "    return stem\n",
    "\n",
    "df['stemmed_text'] = df['Text'].progress_apply(stem_text)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('wordnet')\n",
    "# nltk.download('omw-1.4')\n",
    "\n",
    "def lemmatize_text(corpus):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return [' '.join([lemmatizer.lemmatize(word) for word in tweet.split()]) for tweet in corpus]\n",
    "\n",
    "df['lemmatized_text'] = lemmatize_text(df['Text'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopwords Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = stopwords.words('english')\n",
    "additional_stopwords = [\"'s\",\"...\",\"'ve\",\"``\",\"''\",\"'m\",'--',\"'ll\",\"'d\", 'u', 'b', 'c', 'd', 'x', 'xf', 'f', 'p', 'xb']\n",
    "stop = set(stop + additional_stopwords)\n",
    "\n",
    "def remove_stopwords(x):\n",
    "    x = word_tokenize(x)\n",
    "    store_words = ''\n",
    "    \n",
    "    for i in x:\n",
    "        if i not in stop:\n",
    "            store_words += i + ' '\n",
    "            \n",
    "    return store_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cleaned_text'] = df['Text'].progress_apply(remove_stopwords)\n",
    "df['cleaned_stemmed_text'] = df['stemmed_text'].progress_apply(remove_stopwords)\n",
    "df['cleaned_lemmatized_text'] = df['lemmatized_text'].progress_apply(remove_stopwords)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"1.3\"></a>\n",
    "[<font size=\"+1\"><b>1.3. Data Visualization</b></font><br>](#0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"========== number of words in the corpus ==========\")\n",
    "\n",
    "print(f\"original: {df['Text'].apply(lambda x: len(x.split())).sum()}\")\n",
    "\n",
    "print(f\"after stemming: {df['stemmed_text'].apply(lambda x: len(x.split())).sum()}\")\n",
    "\n",
    "print(f\"after lemmatization: {df['lemmatized_text'].apply(lambda x: len(x.split())).sum()}\")\n",
    "\n",
    "print(f\"after removal of stopwords: {df['cleaned_text'].apply(lambda x: len(x.split())).sum()}\")\n",
    "\n",
    "print(f\"after stemming and removal of stopwords: {df['cleaned_stemmed_text'].apply(lambda x: len(x.split())).sum()}\")\n",
    "\n",
    "print(f\"after lemmatization and removal of stopwords: {df['cleaned_lemmatized_text'].apply(lambda x: len(x.split())).sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words_train = df['cleaned_lemmatized_text'].apply(lambda x: len(x.split()))\n",
    "plt.figure(figsize=(12,6))\n",
    "p1=sns.kdeplot(df['cleaned_lemmatized_text'].apply(lambda x: len(x.split())), fill=True, color=\"r\").set_title('Distribution of Number of Words in Corpus (After Lemmatization and Removal of Stopwords)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list_train = df.cleaned_lemmatized_text.str.split()\n",
    "top = Counter([item for sublist in word_list_train for item in sublist])\n",
    "temp = pd.DataFrame(top.most_common(15))\n",
    "temp.columns = ['Common_words','count']\n",
    "temp.style.background_gradient(cmap='Blues')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_wordcloud(text, mask=None, max_words=200, max_font_size=100, figure_size=(24.0,16.0), color = 'white',\n",
    "                   title = None, title_size=40, image_color=False):\n",
    "\n",
    "    wordcloud = WordCloud(background_color=color,\n",
    "                    stopwords = STOPWORDS,\n",
    "                    max_words = max_words,\n",
    "                    max_font_size = max_font_size, \n",
    "                    random_state = 42,\n",
    "                    width=400, \n",
    "                    height=200,\n",
    "                    mask = mask)\n",
    "    wordcloud.generate(str(text))\n",
    "    \n",
    "    plt.figure(figsize=figure_size)\n",
    "    if image_color:\n",
    "        image_colors = ImageColorGenerator(mask)\n",
    "        plt.imshow(wordcloud.recolor(color_func=image_colors), interpolation=\"bilinear\")\n",
    "        plt.title(title, fontdict={'size': title_size,  \n",
    "                                  'verticalalignment': 'bottom'})\n",
    "    else:\n",
    "        plt.imshow(wordcloud)\n",
    "        plt.title(title, fontdict={'size': title_size, 'color': 'black', \n",
    "                                  'verticalalignment': 'bottom'})\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_mask = np.array(Image.new('RGB', (1000,600)))\n",
    "plot_wordcloud(word_list_train, mask=pos_mask,color='white',max_font_size=100,title_size=30,title=\"WordCloud of Corpus\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Naive Bayes Classification**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naiveBayesClassification(x_train, x_test, y_train, y_test, preprocessing):\n",
    "\n",
    "    naiveBayes_model = MultinomialNB().fit(x_train, y_train)\n",
    "    result = naiveBayes_model.predict(x_test)\n",
    "    confuse = metrics.confusion_matrix(y_test, result)\n",
    "    \n",
    "    \n",
    "    # ==================================================\n",
    "    # Confusion Matrix\n",
    "    # ==================================================\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    sns.heatmap(confuse, annot = True, fmt='d')\n",
    "    \n",
    "    print(\"==================================================\")\n",
    "    print(\"Model: Naive Bayes Classification\")\n",
    "    print(\"Preprocessing Function: \", preprocessing)\n",
    "    print(\"==================================================\")\n",
    "    print()\n",
    "    plt.title(\"Confusion matrix of Naive Bayes Classification of Tweets\")\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.show()\n",
    "    \n",
    "    # ==================================================\n",
    "    # Evaluation Metrics\n",
    "    # ==================================================\n",
    "    \n",
    "    F1_score = metrics.f1_score(y_test, result)\n",
    "    precision_score = metrics.precision_score(y_test, result)\n",
    "    recall_score = metrics.recall_score(y_test, result)\n",
    "    average_precision = metrics.average_precision_score(y_test, result)\n",
    "    \n",
    "    print(\"------------ Evaluation Metrics ----------------\")\n",
    "    print()\n",
    "    print('F1 score: {0:0.3f}'.format(F1_score))\n",
    "    print('Precision score: {0:0.3f}'.format(precision_score))\n",
    "    print('Recall score: {0:0.3f}'.format(recall_score))\n",
    "    print('Average precision-recall score: {0:0.3f}'.format(average_precision))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **K-Nearest Neighbour (KNN) Classification**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KNNClassification(x_train, x_test, y_train, y_test, preprocessing):\n",
    "    \n",
    "    metric = ['euclidean', 'manhattan', 'cosine']\n",
    "    neighbors = list(range(1,30))\n",
    "    leaf_size = list(range(1,50))\n",
    "    weight = ['uniform', 'distance']\n",
    "    param_grid = {'metric' : metric, 'n_neighbors': neighbors, 'weights' : weight, 'leaf_size' : leaf_size}\n",
    "    \n",
    "    grid_search = GridSearchCV(KNeighborsClassifier(), param_grid, cv=3)\n",
    "    grid_search.fit(x_train, y_train)\n",
    "    \n",
    "    \n",
    "    knn_model = KNeighborsClassifier(n_neighbors = grid_search.best_params_['n_neighbors'], metric= grid_search.best_params_['metric'], leaf_size = grid_search.best_params_['leaf_size'], weights = grid_search.best_params_['weights'])\n",
    "    knn_model.fit(x_train, y_train)   \n",
    "    \n",
    "    result = knn_model.predict(x_test)\n",
    "    confuse = metrics.confusion_matrix(y_test, result)\n",
    "    \n",
    "    # ==================================================\n",
    "    # Confusion Matrix\n",
    "    # ==================================================\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    sns.heatmap(confuse, annot = True, fmt='d')\n",
    "    \n",
    "    print(\"==================================================\")\n",
    "    print(\"Model: K-Nearest Neighbour Classification\")\n",
    "    print(\"Best parameters:\", grid_search.best_params_)\n",
    "    print(\"Preprocessing Function: \", preprocessing)\n",
    "    print(\"==================================================\")\n",
    "    print()\n",
    "    plt.title(\"Confusion matrix of KNN Classification of tweets\")\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    # ==================================================\n",
    "    # Evaluation Metrics\n",
    "    # ==================================================\n",
    "    \n",
    "    F1_score = metrics.f1_score(y_test, result)\n",
    "    precision_score= metrics.precision_score(y_test, result)\n",
    "    recall_score = metrics.recall_score(y_test, result)\n",
    "    average_precision = metrics.average_precision_score(y_test, result)\n",
    "\n",
    "    print(\"------------ Evaluation Metrics ----------------\")\n",
    "    print()\n",
    "    print('F1 score: {0:0.3f}'.format(F1_score))\n",
    "    print('Precision score: {0:0.3f}'.format(precision_score))\n",
    "    print('Recall score: {0:0.3f}'.format(recall_score))\n",
    "    print('Average precision-recall score: {0:0.3f}'.format(average_precision))\n",
    "    print()\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Support Vector Machine (SVM) Classification**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SVMClassification(x_train, x_test, y_train, y_test, preprocessing):\n",
    "\n",
    "    Cs = [0.001, 0.01, 0.1, 1, 10]\n",
    "    gammas = [0.05, 0.1, 0.15, 0.20, 0.25]\n",
    "    degrees = [0, 1, 2, 3, 4, 5, 6]\n",
    "    kernels = ['rbf', 'linear', 'poly']\n",
    "    param_grid = {'C': Cs, 'gamma' : gammas, 'degree' : degrees, 'kernel' : kernels}\n",
    "    \n",
    "    grid_search = GridSearchCV(svm.SVC(), param_grid, cv = 3)\n",
    "    grid_search.fit(x_train, y_train)\n",
    "\n",
    "    SVM_model = svm.SVC(C = grid_search.best_params_['C'], kernel = grid_search.best_params_['kernel'], gamma = grid_search.best_params_['gamma'], degree = grid_search.best_params_['degree'])\n",
    "    SVM_model.fit(x_train, y_train)\n",
    "    \n",
    "    result = SVM_model.predict(x_test)\n",
    "    confuse = metrics.confusion_matrix(y_test, result)\n",
    "    \n",
    "    \n",
    "    # ==================================================\n",
    "    # Confusion Matrix\n",
    "    # ==================================================\n",
    "    fig = plt.figure()\n",
    "    sns.heatmap(confuse, annot = True, fmt='d')\n",
    "    \n",
    "    print(\"==================================================\")\n",
    "    print(\"Model: Support Vector Machine Classification\")\n",
    "    print(\"Preprocessing Function: \", preprocessing)\n",
    "    print(\"Best parameters:\", grid_search.best_params_)\n",
    "    print(\"==================================================\")\n",
    "    print()\n",
    "    plt.title(\"Confusion matrix of SVM Classification of Tweets\")\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    # ==================================================\n",
    "    # Evaluation Metrics\n",
    "    # ==================================================\n",
    "    \n",
    "    F1_score = metrics.f1_score(y_test, result)\n",
    "    precision_score= metrics.precision_score(y_test, result)\n",
    "    recall_score = metrics.recall_score(y_test, result)\n",
    "    average_precision = metrics.average_precision_score(y_test, result)\n",
    "\n",
    "    print(\"------------ Evaluation Metrics ----------------\")\n",
    "    print()\n",
    "    print('F1 score: {0:0.3f}'.format(F1_score))\n",
    "    print('Precision score: {0:0.3f}'.format(precision_score))\n",
    "    print('Recall score: {0:0.3f}'.format(recall_score))\n",
    "    print('Average precision-recall score: {0:0.3f}'.format(average_precision))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Decision Tree Classification**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decisionTreeClassification(x_train, x_test, y_train, y_test, preprocessing):\n",
    "    \n",
    "    clf = DecisionTreeClassifier() \n",
    "\n",
    "    param_grid = { \n",
    "        'criterion': ['gini','entropy'],\n",
    "        'splitter': ['best','random'],\n",
    "        'max_features': ['sqrt','log2']\n",
    "    }\n",
    "    \n",
    "    grid_search = GridSearchCV(clf, param_grid, cv= 3)\n",
    "    grid_search.fit(x_train, y_train)\n",
    "    grid_search.best_params_\n",
    "\n",
    "    decisionTree_model = DecisionTreeClassifier(criterion = grid_search.best_params_['criterion'],splitter = grid_search.best_params_['splitter'], max_features = grid_search.best_params_['max_features'])\n",
    "    decisionTree_model.fit(x_train, y_train)\n",
    "\n",
    "    result = decisionTree_model.predict(x_test)\n",
    "    confuse = metrics.confusion_matrix(y_test, result)\n",
    "\n",
    "\n",
    "    # ==================================================\n",
    "    # Confusion Matrix\n",
    "    # ==================================================\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    sns.heatmap(confuse, annot = True, fmt='d')\n",
    "    \n",
    "    print(\"==================================================\")\n",
    "    print(\"Model: Decision Tree Classification\")\n",
    "    print(\"Preprocessing Function: \", preprocessing)\n",
    "    print(\"Best parameters:\", grid_search.best_params_)\n",
    "    print(\"==================================================\")\n",
    "    print()\n",
    "    plt.title(\"Confusion matrix of Decision Tree Classification of Tweets\")\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.show()\n",
    "\n",
    "    # ==================================================\n",
    "    # Evaluation Metrics\n",
    "    # ==================================================\n",
    "    \n",
    "    F1_score = metrics.f1_score(y_test, result)\n",
    "    precision_score= metrics.precision_score(y_test, result)\n",
    "    recall_score = metrics.recall_score(y_test, result)\n",
    "    average_precision = metrics.average_precision_score(y_test, result)\n",
    "\n",
    "    print(\"------------ Evaluation Metrics ----------------\")\n",
    "    print()\n",
    "    print('F1 score: {0:0.3f}'.format(F1_score))\n",
    "    print('Precision score: {0:0.3f}'.format(precision_score))\n",
    "    print('Recall score: {0:0.3f}'.format(recall_score))\n",
    "    print('Average precision-recall score: {0:0.3f}'.format(average_precision))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***Subjectivity Classification*** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "\n",
    "preprocessings = ['Text', 'stemmed_text', 'cleaned_stemmed_text', 'lemmatized_text', 'cleaned_lemmatized_text', 'cleaned_text'] \n",
    "#Count Vectorizer\n",
    "for preprocessing in preprocessings:\n",
    "    x_train, x_test, y_train, y_test = train_test_split(df[[preprocessing]], \n",
    "                                                        df['Subjectivity'],\n",
    "                                                        test_size = 0.2, \n",
    "                                                        random_state = 24)\n",
    "    \n",
    "    vectorizer = CountVectorizer(ngram_range=(1,2))\n",
    "    tweet_matrix = vectorizer.fit_transform(x_train[preprocessing])\n",
    "    test_vector = vectorizer.transform(x_test[preprocessing])\n",
    "    \n",
    "    naiveBayesClassification(tweet_matrix, test_vector, y_train, y_test, preprocessing)\n",
    "    KNNClassification(tweet_matrix, test_vector, y_train, y_test, preprocessing)\n",
    "    SVMClassification(tweet_matrix, test_vector, y_train, y_test, preprocessing)\n",
    "    decisionTreeClassification(tweet_matrix, test_vector, y_train, y_test, preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TF-IDF\n",
    "for preprocessing in preprocessings:\n",
    "    x_train, x_test, y_train, y_test = train_test_split(df[[preprocessing]], \n",
    "                                                        df['Subjectivity'],\n",
    "                                                        test_size = 0.2, \n",
    "                                                        random_state = 24)\n",
    "    vectorizer = TfidfVectorizer(use_idf = True, sublinear_tf = True)\n",
    "    tweet_matrix = vectorizer.fit_transform(x_train[preprocessing])\n",
    "    test_vector = vectorizer.transform(x_test[preprocessing])\n",
    "    naiveBayesClassification(tweet_matrix, test_vector, y_train, y_test, preprocessing)\n",
    "    KNNClassification(tweet_matrix, test_vector, y_train, y_test, preprocessing)\n",
    "    SVMClassification(tweet_matrix, test_vector, y_train, y_test, preprocessing)\n",
    "    decisionTreeClassification(tweet_matrix, test_vector, y_train, y_test, preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best F1 score with cleaned_text & count vectorizer\n",
    "preprocessing = 'cleaned_text'\n",
    "x_train, x_test, y_train, y_test = train_test_split(df[[preprocessing]], \n",
    "                                                    df['Subjectivity'],\n",
    "                                                    test_size = 0.2, \n",
    "                                                    random_state = 24)\n",
    "    \n",
    "vectorizer = CountVectorizer(ngram_range=(1,2))\n",
    "tweet_matrix = vectorizer.fit_transform(x_train[preprocessing])\n",
    "test_vector = vectorizer.transform(x_test[preprocessing])\n",
    "    \n",
    "naiveBayesClassification(tweet_matrix, test_vector, y_train, y_test, preprocessing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Polarity Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df[\"Polarity\"].notna()]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessings = ['Text', 'stemmed_text', 'cleaned_stemmed_text', 'lemmatized_text', 'cleaned_lemmatized_text', 'cleaned_text'] \n",
    "#Count Vectorizer\n",
    "for preprocessing in preprocessings:\n",
    "    x_train, x_test, y_train, y_test = train_test_split(df[[preprocessing]], \n",
    "                                                        df['Polarity'],\n",
    "                                                        test_size = 0.2, \n",
    "                                                        random_state = 24)\n",
    "    \n",
    "    vectorizer = CountVectorizer(ngram_range=(1,2))\n",
    "    tweet_matrix = vectorizer.fit_transform(x_train[preprocessing])\n",
    "    test_vector = vectorizer.transform(x_test[preprocessing])\n",
    "    \n",
    "    naiveBayesClassification(tweet_matrix, test_vector, y_train, y_test, preprocessing)\n",
    "    KNNClassification(tweet_matrix, test_vector, y_train, y_test, preprocessing)\n",
    "    SVMClassification(tweet_matrix, test_vector, y_train, y_test, preprocessing)\n",
    "    decisionTreeClassification(tweet_matrix, test_vector, y_train, y_test, preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TF-IDF\n",
    "for preprocessing in preprocessings:\n",
    "    x_train, x_test, y_train, y_test = train_test_split(df[[preprocessing]], \n",
    "                                                        df['Polarity'],\n",
    "                                                        test_size = 0.2, \n",
    "                                                        random_state = 24)\n",
    "    \n",
    "    vectorizer = TfidfVectorizer(use_idf = True, sublinear_tf = True)\n",
    "    tweet_matrix = vectorizer.fit_transform(x_train[preprocessing])\n",
    "    test_vector = vectorizer.transform(x_test[preprocessing])\n",
    "    \n",
    "    naiveBayesClassification(tweet_matrix, test_vector, y_train, y_test, preprocessing)\n",
    "    KNNClassification(tweet_matrix, test_vector, y_train, y_test, preprocessing)\n",
    "    SVMClassification(tweet_matrix, test_vector, y_train, y_test, preprocessing)\n",
    "    decisionTreeClassification(tweet_matrix, test_vector, y_train, y_test, preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best F1 score with cleaned_lemmatized_text & count vectorizer\n",
    "preprocessing = 'cleaned_lemmatized_text'\n",
    "x_train, x_test, y_train, y_test = train_test_split(df[[preprocessing]], \n",
    "                                                    df['Polarity'],\n",
    "                                                    test_size = 0.2, \n",
    "                                                    random_state = 24)\n",
    "    \n",
    "vectorizer = CountVectorizer(ngram_range=(1,2))\n",
    "tweet_matrix = vectorizer.fit_transform(x_train[preprocessing])\n",
    "test_vector = vectorizer.transform(x_test[preprocessing])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2d40b353eadc8cbb987c2ca5c7db5aac5278416995cd628ee8f7b8b865cb97db"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
