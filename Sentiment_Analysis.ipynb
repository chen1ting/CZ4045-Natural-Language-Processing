{"cells":[{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"0ab5af52-4622-4f57-832a-88b4770428b0","showTitle":false,"title":""},"id":"oIYClDQLWu6x"},"source":["# Sentiment Analysis"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"3c9105c9-d524-41f8-a9f9-d12534087d15","showTitle":false,"title":""},"id":"lP7EohXPWu61"},"source":["<a id=\"0\"></a>\n","#### Contents:\n","* [1. Dataset](#1)\n","    * [1.1. Loading Data](#1.1)\n","    * [1.2. Data Preprocessing](#1.2)\n","    * [1.3. Dataset Visualization](#1.3)\n","* [2. Classifiers](#2)\n","    * [2.1. Naive Bayes Classfication](#2.1)\n","    * [2.2. K-Nearest Neighbour Classification](#2.2)\n","    * [2.3. Support Vector Machine (SVM) Classification](#2.3)\n","    * [2.4. Decision Tree Machine Classification](#2.3)\n","* [Advanced. Emotions in data](#advanced)"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"91becc97-2455-4250-9466-eeb073cc6976","showTitle":false,"title":""},"id":"kiMqQUONXiIs"},"outputs":[],"source":["# !pip install langdetect\n","# !pip install regex\n","# !pip install tqdm\n","# !pip install nltk\n","# !pip install imblearn\n","# !pip install tensorflow\n","# !pip install transformers\n","# !pip install wordcloud\n","# !pip install jinja2"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"dfa34752-11ef-4e9e-9205-557e77bf4bea","showTitle":false,"title":""},"id":"_6XuoYYvWu63"},"outputs":[],"source":["# Import libraries\n","import pandas as pd\n","import numpy as np\n","import regex as re\n","import string\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","import seaborn as sns\n","import time\n","import random\n","from tqdm import tqdm\n","tqdm.pandas()\n","\n","# preprocessing \n","import nltk\n","from nltk.corpus import wordnet as wn\n","from nltk.stem import PorterStemmer\n","from nltk.stem import WordNetLemmatizer\n","from nltk.tokenize import word_tokenize\n","from nltk.corpus import stopwords\n","from langdetect import detect\n","from langdetect import DetectorFactory\n","from imblearn.under_sampling import RandomUnderSampler\n","\n","# word cloud\n","from PIL import Image\n","from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n","pd.options.mode.chained_assignment = None\n","\n","\n","from sklearn import metrics\n","from sklearn import svm\n","from sklearn.model_selection import train_test_split\n","from sklearn.model_selection import StratifiedShuffleSplit\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.model_selection import cross_val_score\n","\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.feature_extraction.text import TfidfTransformer\n","from sklearn.decomposition import PCA\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.ensemble import RandomForestClassifier\n","# metrics\n","from sklearn.metrics import accuracy_score, f1_score\n","from sklearn.metrics import classification_report, confusion_matrix\n","from collections import Counter\n","\n","import warnings\n","warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n","# warnings.filterwarnings(action='ignore')    # to suppress future warnings etc\n","\n","# Below are for Bert \n","\n","# transformers\n","from transformers import BertTokenizerFast\n","from transformers import TFBertModel\n","\n","# keras\n","import tensorflow as tf\n","from tensorflow import keras\n","\n","# set seed for reproducibility\n","seed=42"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"a5e4ef32-8c4a-4369-b41f-c30df0d3c6cb","showTitle":false,"title":""},"id":"l0iy4hLtWu66"},"source":["<a id=\"1\"></a>\n","[<font size=\"+2\" ><b>1. Dataset Presentation</b></font><br>](#0)\n","\n","It is wise to explore our dataset first"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"92574e0c-7ec7-4be0-ba9a-072ae566b6ce","showTitle":false,"title":""},"id":"A0Vk182KWu67"},"source":["<a id=\"1.1\"></a>\n","[<font size=\"+1\" ><b>1.1. Loading Data</b></font><br>](#0)\n","Load necessary data."]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"f179bb67-136d-430a-963c-d48db28ab348","showTitle":false,"title":""},"id":"HhDbQgDFWu67"},"outputs":[],"source":["# Removing unused columns\n","df_outside = pd.read_csv('dataset_transformed.csv', encoding='latin')\n","df_crawl = pd.read_csv('compiled_tweets.csv')\n","\n","# df_outside = pd.read_csv(\"/dbfs/FileStore/shared_uploads/dataset_transformed.csv\", encoding='latin')\n","# df_crawl = pd.read_csv(\"/dbfs/FileStore/shared_uploads/compiled_tweets.csv\", encoding='latin')\n","df_outside = df_outside[[\"Text\", \"Subjectivity\", \"Polarity\"]]\n","df_crawl = df_crawl[[\"Text\", \"Subjectivity\", \"Polarity\"]]\n","df_crawl = df_crawl[df_crawl['Subjectivity'].notna()].copy()\n","df = pd.concat([df_crawl, df_outside]).reset_index(drop=True)\n","df.info()"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"04e9dfcd-a4ca-4719-86dc-3bdb2fc62edf","showTitle":false,"title":""},"id":"lhlu5jY0Wu68"},"source":["<a id=\"1.2\"></a>\n","[<font size=\"+1\" ><b>1.2. Data Preprocessing</b></font><br>](#0)\n","\n","As twitter texts are relatively unclean compared with others, special cleaning are applied to remove emojis and links"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"5b3a6fff-f943-491f-b8e4-209e697aef75","showTitle":false,"title":""},"id":"qljG_N1xWu68"},"outputs":[],"source":["# set seed\n","DetectorFactory.seed = 0\n","\n","def language_detection(x:str):\n","    text = x.split(\" \")\n","    \n","    lang = \"en\"\n","    try:\n","        if len(text) > 50:\n","            lang = detect(\" \".join(text[:50]))\n","        elif len(text) > 0:\n","            lang = detect(\" \".join(text[:len(text)]))\n","    except Exception as e:\n","        all_words = set(text)\n","        try:\n","            lang = detect(\" \".join(all_words))\n","        except Exception as e:\n","            lang = \"unknown\"\n","            pass\n","    return lang"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"13bf54dc-8d91-4346-ad25-09d4b60851d9","showTitle":false,"title":""},"colab":{"base_uri":"https://localhost:8080/"},"id":"ZhB2fch_Wu69","outputId":"8bfd34e1-e3ae-463d-f265-1737b9bfde5f"},"outputs":[],"source":["df['Language'] = df['Text'].progress_apply(language_detection)"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"78239b29-f92c-4ba9-b07d-94a56bec51e0","showTitle":false,"title":""},"colab":{"base_uri":"https://localhost:8080/","height":272},"id":"pvFjrNXeWu6_","outputId":"0f5ccbd4-c4f2-4852-ad3c-ca6f69f7d7c0"},"outputs":[],"source":["#plot the distribution of language\n","fig = plt.figure(figsize=(15, 5))\n","sns.histplot(data=df['Language'])\n","plt.title(\"Distribution of Language\")\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"f4d3adcd-69e0-4a86-988a-7998a727963e","showTitle":false,"title":""},"id":"kjKBiaFgWu6_"},"outputs":[],"source":["df = df[df['Language'] == 'en'].copy()"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"d997000a-4ec2-46a0-97a8-8d993f3fcd1f","showTitle":false,"title":""},"id":"mhawX21RWu7A"},"outputs":[],"source":["def clean_text(text):\n","    text = str(text)\n","    text = re.sub(r'[^a-zA-Z ]+', ' ', text)\n","    text = re.sub(r'http\\S+', ' ', text)\n","    text = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', text)\n","    text = re.sub(r'^RT[\\s]+', '', text)\n","    # text = re.sub(r'pic.twitter\\S+', ' ', text)\n","    text = re.sub(r'#', '', text)\n","    text = text.lower()\n","\n","    return text\n","\n","def decontracted(text):\n","    text = re.sub(r\"won\\'t\", \"will not\", text)\n","    text = re.sub(r\"don't\", \"do not\", text)\n","    text = re.sub(r\"don't\", \"do not\", text)\n","    text = re.sub(r\"can\\'t\", \"can not\", text)\n","    text = re.sub(r\"n\\'t\", \" not\", text)\n","    text = re.sub(r\"\\'re\", \" are\", text)\n","    text = re.sub(r\"it\\'s\", \"it is\", text)\n","    text = re.sub(r\"\\'d\", \" would\", text)\n","    text = re.sub(r\"\\'ll\", \" will\", text)\n","    text = re.sub(r\"\\'t\", \" not\", text)\n","    text = re.sub(r\"\\'ve\", \" have\", text)\n","    text = re.sub(r\"\\'m\", \" am\", text)\n","\n","    text = re.sub(r\"n\\'t\", \" not\", text)\n","    text = re.sub(r\"\\'re\", \" are\", text)\n","    text = re.sub(r\"\\'re\", \" are\", text)\n","    text = re.sub(r\"\\'d\", \" would\", text)\n","    text = re.sub(r\"\\'d\", \" would\", text)\n","    text = re.sub(r\"\\'ll\", \" will\", text)\n","    text = re.sub(r\"\\'ll\", \" will\", text)\n","    text = re.sub(r\"\\'t\", \" not\", text)\n","    text = re.sub(r\"\\'t\", \" not\", text)\n","    text = re.sub(r\"\\'ve\", \" have\", text)\n","    text = re.sub(r\"\\'ve\", \" have\", text)\n","    text = re.sub(r\"\\'m\", \" am\", text)\n","    text = re.sub(r\"\\'m\", \" am\", text)\n","    text = re.sub(r\"\\“\", \"\", text)\n","    text = re.sub(r\"\\”\", \"\", text)\n","    text = re.sub(r\"\\…\", \"\", text)\n","\n","    return text\n","\n","\n","def remove_punc(tweet):\n","    tweet =  tweet.translate(str.maketrans('', '', string.punctuation))\n","    tweet = ' '.join([word for word in tweet.split()])\n","    tweet = tweet.lower()\n","    \n","    return tweet"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"179f07d1-33a3-4c5b-a0f2-5e3e66f68807","showTitle":false,"title":""},"colab":{"base_uri":"https://localhost:8080/","height":223},"id":"LgfpC-R1Wu7B","outputId":"3b5f1f05-57e0-4456-ff72-a1b6b94cf63c"},"outputs":[],"source":["df['Text'] = df['Text'].progress_apply(clean_text).apply(decontracted).apply(remove_punc)\n","df.head()"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"84b8cfcc-af3f-4340-8308-e5580955881d","showTitle":false,"title":""},"id":"I-GMhsHtWu7B"},"source":["### Stemming"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"788b436c-3d63-4b1c-82a5-a6c2d5cc9cc5","showTitle":false,"title":""},"colab":{"base_uri":"https://localhost:8080/","height":424},"id":"XTT8TBrjWu7B","outputId":"0791fcdf-7b96-458a-8599-eaa2cf651903"},"outputs":[],"source":["nltk.download('punkt')\n","nltk.download('stopwords')\n","\n","def stem_text(x):\n","    stemmer = PorterStemmer()\n","    x = word_tokenize(x)\n","    stem = ''\n","    \n","    for i in x:\n","        stem += stemmer.stem(i) + ' '\n","        \n","    return stem\n","\n","df['stemmed_text'] = df['Text'].progress_apply(stem_text)\n","df.head()"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"622db879-688d-4781-8cee-333a0805eb36","showTitle":false,"title":""},"id":"6EvLvH8AWu7C"},"source":["### Lemmatization"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"db25b486-60e8-47fc-800a-50e69dff5359","showTitle":false,"title":""},"colab":{"base_uri":"https://localhost:8080/","height":510},"id":"mc-u_bleWu7C","outputId":"c5d8949a-cdcc-487b-d68f-4e547503ba72"},"outputs":[],"source":["nltk.download('wordnet')\n","nltk.download('omw-1.4')\n","\n","def lemmatize_text(corpus):\n","    lemmatizer = WordNetLemmatizer()\n","    return [' '.join([lemmatizer.lemmatize(word) for word in tweet.split()]) for tweet in corpus]\n","\n","df['lemmatized_text'] = lemmatize_text(df['Text'])\n","df.head()"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"2292d43a-f0bf-4021-a87d-9916e1d2153d","showTitle":false,"title":""},"id":"m1ZClqHnWu7D"},"source":["### Stopwords Removal"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"072d5645-b0e9-4c0d-971c-5ce21f2be4b7","showTitle":false,"title":""},"id":"gaFNPKpRWu7I"},"outputs":[],"source":["stop = stopwords.words('english')\n","additional_stopwords = [\"'s\",\"...\",\"'ve\",\"``\",\"''\",\"'m\",'--',\"'ll\",\"'d\", 'u', 'b', 'c', 'd', 'x', 'xf', 'f', 'p', 'xb']\n","stop = set(stop + additional_stopwords)\n","\n","def remove_stopwords(x):\n","    x = word_tokenize(x)\n","    store_words = ''\n","    \n","    for i in x:\n","        if i not in stop:\n","            store_words += i + ' '\n","            \n","    return store_words"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"b79acbf7-75fc-4edf-95e5-daff5ad96c61","showTitle":false,"title":""},"colab":{"base_uri":"https://localhost:8080/","height":617},"id":"tsO1Ia5AWu7J","outputId":"5032c6a5-f72b-4eb0-9ff9-f685881e7384"},"outputs":[],"source":["df['cleaned_text'] = df['Text'].progress_apply(remove_stopwords)\n","df['cleaned_stemmed_text'] = df['stemmed_text'].progress_apply(remove_stopwords)\n","df['cleaned_lemmatized_text'] = df['lemmatized_text'].progress_apply(remove_stopwords)\n","\n","df.head()"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"d82f0c1e-f843-4a0a-b27e-3e6cd9e6876c","showTitle":false,"title":""},"id":"HxLYrzSJWu7J"},"source":["<a id=\"1.3\"></a>\n","[<font size=\"+1\"><b>1.3. Data Visualization</b></font><br>](#0)"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"703abe47-9dc1-40a7-860c-ccb4dc4457bd","showTitle":false,"title":""},"colab":{"base_uri":"https://localhost:8080/"},"id":"ArnrsiQrWu7J","outputId":"a5a134a8-57b6-4259-c9e8-10b02df67391"},"outputs":[],"source":["print(\"========== number of words in the corpus ==========\")\n","\n","print(f\"original: {df['Text'].apply(lambda x: len(x.split())).sum()}\")\n","\n","print(f\"after stemming: {df['stemmed_text'].apply(lambda x: len(x.split())).sum()}\")\n","\n","print(f\"after lemmatization: {df['lemmatized_text'].apply(lambda x: len(x.split())).sum()}\")\n","\n","print(f\"after removal of stopwords: {df['cleaned_text'].apply(lambda x: len(x.split())).sum()}\")\n","\n","print(f\"after stemming and removal of stopwords: {df['cleaned_stemmed_text'].apply(lambda x: len(x.split())).sum()}\")\n","\n","print(f\"after lemmatization and removal of stopwords: {df['cleaned_lemmatized_text'].apply(lambda x: len(x.split())).sum()}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"6b3361f7-f161-4b47-a603-50fb9b172b45","showTitle":false,"title":""},"colab":{"base_uri":"https://localhost:8080/","height":386},"id":"Q23c-oSdWu7K","outputId":"9c8dfc30-6afe-4122-d381-3641c4a50db0"},"outputs":[],"source":["num_words_train = df['cleaned_lemmatized_text'].apply(lambda x: len(x.split()))\n","plt.figure(figsize=(12,6))\n","p1=sns.kdeplot(df['cleaned_lemmatized_text'].apply(lambda x: len(x.split())), fill=True, color=\"r\").set_title('Distribution of Number of Words in Corpus (After Lemmatization and Removal of Stopwords)')"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"6e5d14a1-3c83-478f-a581-7be92e0645eb","showTitle":false,"title":""},"colab":{"base_uri":"https://localhost:8080/","height":520},"id":"YzQJzZzBWu7K","outputId":"c7e66148-6e3d-4948-f421-3cfb50d345b8"},"outputs":[],"source":["word_list_train = df.cleaned_lemmatized_text.str.split()\n","top = Counter([item for sublist in word_list_train for item in sublist])\n","temp = pd.DataFrame(top.most_common(15))\n","temp.columns = ['Common_words','count']\n","temp.style.background_gradient(cmap='Blues')"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"1175c3b2-1d38-49fc-ba9d-b5af7efe2726","showTitle":false,"title":""},"id":"pmZUfQqBWu7L"},"outputs":[],"source":["def plot_wordcloud(text, mask=None, max_words=200, max_font_size=100, figure_size=(24.0,16.0), color = 'white',\n","                   title = None, title_size=40, image_color=False):\n","\n","    wordcloud = WordCloud(background_color=color,\n","                    stopwords = STOPWORDS,\n","                    max_words = max_words,\n","                    max_font_size = max_font_size, \n","                    random_state = 42,\n","                    width=400, \n","                    height=200,\n","                    mask = mask)\n","    wordcloud.generate(str(text))\n","    \n","    plt.figure(figsize=figure_size)\n","    if image_color:\n","        image_colors = ImageColorGenerator(mask)\n","        plt.imshow(wordcloud.recolor(color_func=image_colors), interpolation=\"bilinear\")\n","        plt.title(title, fontdict={'size': title_size,  \n","                                  'verticalalignment': 'bottom'})\n","    else:\n","        plt.imshow(wordcloud)\n","        plt.title(title, fontdict={'size': title_size, 'color': 'black', \n","                                  'verticalalignment': 'bottom'})\n","    plt.axis('off')\n","    plt.tight_layout()"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"25e0a33f-c376-41d3-b8bf-b68045a0a19c","showTitle":false,"title":""},"colab":{"base_uri":"https://localhost:8080/","height":449},"id":"3PeLIbZkWu7L","outputId":"91ec49df-efc4-455a-cb28-f0518f8330ed"},"outputs":[],"source":["pos_mask = np.array(Image.new('RGB', (1000,600)))\n","plot_wordcloud(word_list_train, mask=pos_mask,color='white',max_font_size=100,title_size=30,title=\"WordCloud of Corpus\")"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"7a21e0d3-d24a-4260-9e8e-ee29f4c1517f","showTitle":false,"title":""},"id":"vM5jIFgHZcsU"},"source":["<a id=\"2\"></a>\n","[<font size=\"+2\" ><b>2. Classifiers</b></font><br>](#0)"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"17ab0e89-ac70-4799-b070-7f0fa40bb72a","showTitle":false,"title":""},"id":"pNwAXtQXWu7M"},"source":["<a id=\"2.1\"></a>\n","[<font size=\"+1\" ><b>2.1. Naive Bayes Classification</b></font><br>](#0)"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"89e08262-a351-4ebd-ab1b-027f232d1116","showTitle":false,"title":""},"id":"uy-35B0-Wu7M"},"outputs":[],"source":["def naiveBayesClassification(x_train, x_test, y_train, y_test, preprocessing):\n","\n","    naiveBayes_model = MultinomialNB().fit(x_train, y_train)\n","    result = naiveBayes_model.predict(x_test)\n","    confuse = metrics.confusion_matrix(y_test, result)\n","    \n","    \n","    # ==================================================\n","    # Confusion Matrix\n","    # ==================================================\n","    \n","    fig = plt.figure()\n","    sns.heatmap(confuse, annot = True, fmt='d')\n","    \n","    print(\"==================================================\")\n","    print(\"Model: Naive Bayes Classification\")\n","    print(\"Preprocessing Function: \", preprocessing)\n","    print(\"==================================================\")\n","    print()\n","    plt.title(\"Confusion matrix of Naive Bayes Classification of Tweets\")\n","    plt.xlabel('Predicted')\n","    plt.ylabel('Actual')\n","    plt.show()\n","    \n","    # ==================================================\n","    # Evaluation Metrics\n","    # ==================================================\n","    \n","    F1_score = metrics.f1_score(y_test, result)\n","    precision_score = metrics.precision_score(y_test, result)\n","    recall_score = metrics.recall_score(y_test, result)\n","    average_precision = metrics.average_precision_score(y_test, result)\n","    \n","    print(\"------------ Evaluation Metrics ----------------\")\n","    print()\n","    print('F1 score: {0:0.3f}'.format(F1_score))\n","    print('Precision score: {0:0.3f}'.format(precision_score))\n","    print('Recall score: {0:0.3f}'.format(recall_score))\n","    print('Average precision-recall score: {0:0.3f}'.format(average_precision))\n","    print()"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"7e13414e-fe91-4445-b58f-d66693186088","showTitle":false,"title":""},"id":"5HgTBrG7Wu7N"},"source":["<a id=\"2.2\"></a>\n","[<font size=\"+1\" ><b>2.2. K-Nearest Neighbour (KNN) Classification</b></font><br>](#0)"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"79e65340-1cbf-4646-93e5-3bcd9ba879fe","showTitle":false,"title":""},"id":"-enOMjM0Wu7N"},"outputs":[],"source":["def KNNClassification(x_train, x_valid, x_test, y_train, y_valid, y_test, preprocessing):\n","    \n","    metric = ['euclidean', 'manhattan', 'cosine']\n","    neighbors = list(range(1,30))\n","    leaf_size = list(range(1,50))\n","    weight = ['uniform', 'distance']\n","    param_grid = {'metric' : metric, 'n_neighbors': neighbors, 'weights' : weight, 'leaf_size' : leaf_size}\n","    \n","    grid_search = GridSearchCV(KNeighborsClassifier(), param_grid, cv=3)\n","    grid_search.fit(x_valid, y_valid)\n","    \n","    \n","    knn_model = KNeighborsClassifier(n_neighbors = grid_search.best_params_['n_neighbors'], metric= grid_search.best_params_['metric'], leaf_size = grid_search.best_params_['leaf_size'], weights = grid_search.best_params_['weights'])\n","    knn_model.fit(x_train, y_train)   \n","    \n","    result = knn_model.predict(x_test)\n","    confuse = metrics.confusion_matrix(y_test, result)\n","    \n","    # ==================================================\n","    # Confusion Matrix\n","    # ==================================================\n","    \n","    fig = plt.figure()\n","    sns.heatmap(confuse, annot = True, fmt='d')\n","    \n","    print(\"==================================================\")\n","    print(\"Model: K-Nearest Neighbour Classification\")\n","    print(\"Best parameters:\", grid_search.best_params_)\n","    print(\"Preprocessing Function: \", preprocessing)\n","    print(\"==================================================\")\n","    print()\n","    plt.title(\"Confusion matrix of KNN Classification of tweets\")\n","    plt.xlabel('Predicted')\n","    plt.ylabel('Actual')\n","    plt.show()\n","    \n","    \n","    # ==================================================\n","    # Evaluation Metrics\n","    # ==================================================\n","    \n","    F1_score = metrics.f1_score(y_test, result)\n","    precision_score= metrics.precision_score(y_test, result)\n","    recall_score = metrics.recall_score(y_test, result)\n","    average_precision = metrics.average_precision_score(y_test, result)\n","\n","    print(\"------------ Evaluation Metrics ----------------\")\n","    print()\n","    print('F1 score: {0:0.3f}'.format(F1_score))\n","    print('Precision score: {0:0.3f}'.format(precision_score))\n","    print('Recall score: {0:0.3f}'.format(recall_score))\n","    print('Average precision-recall score: {0:0.3f}'.format(average_precision))\n","    print()\n","  "]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"e969e1db-2f39-45e0-881c-17a626bccb11","showTitle":false,"title":""},"id":"z6lx3i82Wu7O"},"source":["<a id=\"2.3\"></a>\n","[<font size=\"+1\" ><b>2.3. Support Vector Machine (SVM) Classification</b></font><br>](#0)"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"888acd9d-76fe-49b9-8d20-8d9bdae93f12","showTitle":false,"title":""},"id":"iDXxN4DaWu7O"},"outputs":[],"source":["def SVMClassification(x_train, x_valid, x_test, y_train, y_valid, y_test, preprocessing):\n","\n","    Cs = [0.001, 0.01, 0.1, 1, 10]\n","    gammas = [0.05, 0.1, 0.15, 0.20, 0.25]\n","    degrees = [0, 1, 2, 3, 4, 5, 6]\n","    kernels = ['rbf', 'linear', 'poly']\n","    param_grid = {'C': Cs, 'gamma' : gammas, 'degree' : degrees, 'kernel' : kernels}\n","    \n","    grid_search = GridSearchCV(svm.SVC(), param_grid, cv = 3)\n","    grid_search.fit(x_valid, y_valid)\n","\n","    SVM_model = svm.SVC(C = grid_search.best_params_['C'], kernel = grid_search.best_params_['kernel'], gamma = grid_search.best_params_['gamma'], degree = grid_search.best_params_['degree'])\n","    SVM_model.fit(x_train, y_train)\n","    \n","    result = SVM_model.predict(x_test)\n","    confuse = metrics.confusion_matrix(y_test, result)\n","    \n","    \n","    # ==================================================\n","    # Confusion Matrix\n","    # ==================================================\n","    fig = plt.figure()\n","    sns.heatmap(confuse, annot = True, fmt='d')\n","    \n","    print(\"==================================================\")\n","    print(\"Model: Support Vector Machine Classification\")\n","    print(\"Preprocessing Function: \", preprocessing)\n","    print(\"Best parameters:\", grid_search.best_params_)\n","    print(\"==================================================\")\n","    print()\n","    plt.title(\"Confusion matrix of SVM Classification of Tweets\")\n","    plt.xlabel('Predicted')\n","    plt.ylabel('Actual')\n","    plt.show()\n","    \n","    \n","    # ==================================================\n","    # Evaluation Metrics\n","    # ==================================================\n","    \n","    F1_score = metrics.f1_score(y_test, result)\n","    precision_score= metrics.precision_score(y_test, result)\n","    recall_score = metrics.recall_score(y_test, result)\n","    average_precision = metrics.average_precision_score(y_test, result)\n","\n","    print(\"------------ Evaluation Metrics ----------------\")\n","    print()\n","    print('F1 score: {0:0.3f}'.format(F1_score))\n","    print('Precision score: {0:0.3f}'.format(precision_score))\n","    print('Recall score: {0:0.3f}'.format(recall_score))\n","    print('Average precision-recall score: {0:0.3f}'.format(average_precision))\n","    print()"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"efd76987-40e2-4d8c-9f6d-065f620f1665","showTitle":false,"title":""},"id":"uEzhHob2Wu7P"},"source":["<a id=\"2.4\"></a>\n","[<font size=\"+1\" ><b>2.4. Decision Tree Classification</b></font><br>](#0)"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"6e504ef9-4236-463b-b1a9-33961ce9de97","showTitle":false,"title":""},"id":"K1mpyDqWWu7P"},"outputs":[],"source":["def decisionTreeClassification(x_train, x_valid, x_test, y_train, y_valid, y_test, preprocessing):\n","    \n","    clf = DecisionTreeClassifier() \n","\n","    param_grid = { \n","        'criterion': ['gini','entropy'],\n","        'splitter': ['best','random'],\n","        'max_features': ['sqrt','log2']\n","    }\n","    \n","    grid_search = GridSearchCV(clf, param_grid, cv= 3)\n","    grid_search.fit(x_valid, y_valid)\n","    grid_search.best_params_\n","\n","    decisionTree_model = DecisionTreeClassifier(criterion = grid_search.best_params_['criterion'],splitter = grid_search.best_params_['splitter'], max_features = grid_search.best_params_['max_features'])\n","    decisionTree_model.fit(x_train, y_train)\n","\n","    result = decisionTree_model.predict(x_test)\n","    confuse = metrics.confusion_matrix(y_test, result)\n","\n","\n","    # ==================================================\n","    # Confusion Matrix\n","    # ==================================================\n","    \n","    fig = plt.figure()\n","    sns.heatmap(confuse, annot = True, fmt='d')\n","    \n","    print(\"==================================================\")\n","    print(\"Model: Decision Tree Classification\")\n","    print(\"Preprocessing Function: \", preprocessing)\n","    print(\"Best parameters:\", grid_search.best_params_)\n","    print(\"==================================================\")\n","    print()\n","    plt.title(\"Confusion matrix of Decision Tree Classification of Tweets\")\n","    plt.xlabel('Predicted')\n","    plt.ylabel('Actual')\n","    plt.show()\n","\n","    # ==================================================\n","    # Evaluation Metrics\n","    # ==================================================\n","    \n","    F1_score = metrics.f1_score(y_test, result)\n","    precision_score= metrics.precision_score(y_test, result)\n","    recall_score = metrics.recall_score(y_test, result)\n","    average_precision = metrics.average_precision_score(y_test, result)\n","\n","    print(\"------------ Evaluation Metrics ----------------\")\n","    print()\n","    print('F1 score: {0:0.3f}'.format(F1_score))\n","    print('Precision score: {0:0.3f}'.format(precision_score))\n","    print('Recall score: {0:0.3f}'.format(recall_score))\n","    print('Average precision-recall score: {0:0.3f}'.format(average_precision))\n","    print()"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"695b3748-93fb-4c55-8bfc-1dbff383a262","showTitle":false,"title":""},"id":"9K7XqioPWu7P"},"source":["<a id=\"3\"></a>\n","[<font size=\"+2\" ><b>3. Training</b></font><br>](#0)"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"8a1342f9-6611-4c23-88dc-ec6fa39d2efb","showTitle":false,"title":""}},"outputs":[],"source":["# don't consider uncleaned text for now (too much noise)\n","preprocessings = ['cleaned_stemmed_text', 'cleaned_lemmatized_text', 'cleaned_text'] "]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"1d979821-541c-4e0d-9107-78c1844daf86","showTitle":false,"title":""}},"source":["<a id=\"3\"></a>\n","[<font size=\"+1\" ><b>3.1. Vectorization</b></font><br>](#0)"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"08191bcf-4411-4b32-85c9-401148383c89","showTitle":false,"title":""}},"outputs":[],"source":["df = df[df['Subjectivity'].notna()].copy()\n","df['Subjectivity'].value_counts()"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"de8b949c-8952-4d61-a20e-1b885144efcf","showTitle":false,"title":""}},"source":["We can see that here the class is highly unbalanced with too many biased content. For better output, we need to balance the dataset first"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"1c9288fc-5ee3-47c0-a6cd-4695f771b2a1","showTitle":false,"title":""}},"outputs":[],"source":["#  Random oversampling involves randomly selecting examples from the minority class, with replacement, and adding them to the training dataset.\n","sampler = RandomUnderSampler(random_state=seed)\n","X_sub, Y_sub = sampler.fit_resample(df[preprocessings], df['Subjectivity'])\n","Y_sub.value_counts()"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"98983615-724c-41bf-a5f3-bfef49addd17","showTitle":false,"title":""}},"outputs":[],"source":["# do the same for 'Polarity'\n","polar_df = df[df['Polarity'].notna()].copy()\n","polar_df['Polarity'].value_counts()"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"7b382b93-67e1-4955-a0cf-a74b5e51ad53","showTitle":false,"title":""}},"outputs":[],"source":["X_polar, Y_polar = sampler.fit_resample(polar_df[preprocessings], polar_df['Polarity'])\n","Y_polar.value_counts()"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"6b878cb0-2735-4497-ac85-c0e50d1ffef7","showTitle":false,"title":""}},"source":["<b> Train, test, cv split </b>"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"d2d56216-03e9-4c0e-9a15-2bc929256d36","showTitle":false,"title":""}},"outputs":[],"source":["# for subjectivity, get train set, valid set (cross validation), andn test set\n","X_train_valid_sub, X_test_sub, y_train_valid_sub, y_test_sub = train_test_split(X_sub, \n","                                                    Y_sub,\n","                                                    test_size = 0.3, \n","                                                    random_state = seed, stratify=Y_sub)\n","X_train_sub, X_valid_sub, y_train_sub, y_valid_sub = train_test_split(X_train_valid_sub, \n","                                                    y_train_valid_sub,\n","                                                    test_size = 0.1, \n","                                                    random_state = seed, stratify=y_train_valid_sub)\n","\n","print(f\"Shape of X_train: {X_train_sub.shape}\")\n","print(f\"Shape of y_train: {y_train_sub.shape}\")\n","print(f\"Shape of X_valid: {X_valid_sub.shape}\")\n","print(f\"Shape of y_valid: {y_valid_sub.shape}\")\n","print(f\"Shape of X_test: {X_test_sub.shape}\")\n","print(f\"Shape of y_test: {y_test_sub.shape}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"84183513-7b58-43f4-afe1-e284675ba121","showTitle":false,"title":""}},"outputs":[],"source":["# for polarity, get train set, valid set (cross validation), andn test set\n","X_train_valid_polar, X_test_polar, y_train_valid_polar, y_test_polar = train_test_split(X_polar, \n","                                                    Y_polar,\n","                                                    test_size = 0.3, \n","                                                    random_state = seed, stratify=Y_polar)\n","X_train_polar, X_valid_polar, y_train_polar, y_valid_polar = train_test_split(X_train_valid_polar, \n","                                                    y_train_valid_polar,\n","                                                    test_size = 0.1, \n","                                                    random_state = seed, stratify=y_train_valid_polar)"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"7c0dc56c-a6eb-4893-84a5-cde0100fcf5d","showTitle":false,"title":""}},"outputs":[],"source":["vec_X_train_sub = {}\n","vec_X_valid_sub = {}\n","vec_X_test_sub = {}\n","\n","for method in preprocessings:\n","    clf = CountVectorizer(ngram_range=(1,2)).fit(X_train_sub[method])\n","    X_train_cv =  clf.transform(X_train_sub[method])\n","    X_valid_cv = clf.transform(X_valid_sub[method])\n","    X_test_cv = clf.transform(X_test_sub[method])\n","    \n","    tf_transformer = TfidfTransformer(use_idf=True).fit(X_train_cv)\n","    vec_X_train_sub[method] = tf_transformer.transform(X_train_cv)\n","    vec_X_valid_sub[method] = tf_transformer.transform(X_valid_cv)\n","    vec_X_test_sub[method] = tf_transformer.transform(X_test_cv)"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"cb249ea1-e4db-431a-af99-8c20f29df29d","showTitle":false,"title":""}},"outputs":[],"source":["vec_X_train_polar = {}\n","vec_X_valid_polar = {}\n","vec_X_test_polar = {}\n","\n","for method in preprocessings:\n","    clf = CountVectorizer(ngram_range=(1,2)).fit(X_train_polar[method])\n","    X_train_cv =  clf.transform(X_train_polar[method])\n","    X_valid_cv = clf.transform(X_valid_polar[method])\n","    X_test_cv = clf.transform(X_test_polar[method])\n","    \n","    tf_transformer = TfidfTransformer(use_idf=True).fit(X_train_cv)\n","    vec_X_train_polar[method] = tf_transformer.transform(X_train_cv)\n","    vec_X_valid_polar[method] = tf_transformer.transform(X_valid_cv)\n","    vec_X_test_polar[method] = tf_transformer.transform(X_test_cv)"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"6e7f92a4-e3ec-4c08-be6b-c7d3cf422258","showTitle":false,"title":""}},"source":["<a id=\"3.2\"></a>\n","[<font size=\"+1\" ><b>3.2. Baseline</b></font><br>](#0)\n","\n","We use naive bayes model as baseline for each preprocessing method"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"022acaf2-97a0-482b-94cd-d2f8ad3b1d32","showTitle":false,"title":""}},"outputs":[],"source":["for method in preprocessings:\n","    x_train = vec_X_train_sub[method]\n","    x_test = vec_X_test_sub[method]\n","    naiveBayesClassification(x_train, x_test, y_train_sub, y_test_sub, method)"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"cbf4f89e-bcf6-4aff-bd4c-9c43361e4c99","showTitle":false,"title":""}},"outputs":[],"source":["for method in preprocessings:\n","    x_train = vec_X_train_polar[method]\n","    x_test = vec_X_test_polar[method]\n","    naiveBayesClassification(x_train, x_test, y_train_polar, y_test_polar, method)"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"fe525f67-e965-4b82-8b55-5903d223797f","showTitle":false,"title":""}},"source":["<a id=\"3.3\"></a>\n","[<font size=\"+1\" ><b>3.3. Classification with different models</b></font><br>](#0)\n","\n","Since we have saved interim results of preprocessing text, we can see which preprocessing method is best for analysis of tweet text by using both count vectorizor and tfidf"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"8731a487-49a4-4417-8755-867b772dd2ff","showTitle":false,"title":""},"id":"q6TX91K3Wu7Q"},"outputs":[],"source":["for method in preprocessings:\n","    x_train = vec_X_train_sub[method]\n","    x_valid = vec_X_valid_sub[method]\n","    x_test = vec_X_test_sub[method]\n","    print(f\"Shape of X_train: {x_train.shape}\")\n","    print(f\"Shape of y_train: {y_train_sub.shape}\")\n","    print(f\"Shape of X_valid: {x_valid.shape}\")\n","    print(f\"Shape of y_valid: {y_valid_sub.shape}\")\n","    print(f\"Shape of X_test: {x_test.shape}\")\n","    print(f\"Shape of y_test: {y_test_sub.shape}\")\n","    \n","    KNNClassification(x_train, x_valid, x_test, y_train_sub, y_valid_sub, y_test_sub, method)\n","    SVMClassification(x_train, x_valid, x_test, y_train_sub, y_valid_sub, y_test_sub, method)\n","    decisionTreeClassification(x_train, x_valid, x_test, y_train_sub, y_valid_sub, y_test_sub, method)"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"2dc9390b-fe17-4396-bc34-dddad23804a5","showTitle":false,"title":""},"id":"krDvo-JRWu7R"},"outputs":[],"source":["# Best F1 score with cleaned_text & count vectorizer\n","# preprocessing = 'cleaned_text'\n","# x_train, x_test, y_train, y_test = train_test_split(df[[preprocessing]], \n","#                                                     df['Subjectivity'],\n","#                                                     test_size = 0.2, \n","#                                                     random_state = 24)\n","    \n","# vectorizer = CountVectorizer(ngram_range=(1,2))\n","# tweet_matrix = vectorizer.fit_transform(x_train[preprocessing])\n","# test_vector = vectorizer.transform(x_test[preprocessing])\n","    \n","# naiveBayesClassification(tweet_matrix, test_vector, y_train, y_test, preprocessing)"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"ead437ee-5192-4da4-b9bb-4cfba75ad37c","showTitle":false,"title":""},"id":"k05R3Y34Wu7R"},"outputs":[],"source":["for method in preprocessings:\n","    x_train = vec_X_train_polar[method]\n","    x_valid = vec_X_valid_polar[method]\n","    x_test = vec_X_test_polar[method]\n","    print(\"=======\")\n","    print(f\"Shape of X_train: {x_train.shape}\")\n","    print(f\"Shape of y_train: {y_train_polar.shape}\")\n","    print(f\"Shape of X_valid: {x_valid.shape}\")\n","    print(f\"Shape of y_test: {y_valid_polar.shape}\")\n","    print(f\"Shape of X_valid: {x_test.shape}\")\n","    print(f\"Shape of y_test: {y_test_polar.shape}\")\n","    \n","    KNNClassification(x_train, x_valid, x_test, y_train_polar, y_valid_polar, y_test_polar, method)\n","    SVMClassification(x_train, x_valid, x_test, y_train_polar, y_valid_polar, y_test_polar, method)\n","    decisionTreeClassification(x_train, x_valid, x_test, y_train_polar, y_valid_polar, y_test_polar, method)"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"56790749-93e0-41e5-ab70-c45e6e27309f","showTitle":false,"title":""},"id":"Fe45KQlvWu7S"},"outputs":[],"source":["# # Best F1 score with cleaned_lemmatized_text & count vectorizer\n","# preprocessing = 'cleaned_lemmatized_text'\n","# x_train, x_test, y_train, y_test = train_test_split(df[[preprocessing]], \n","#                                                     df['Polarity'],\n","#                                                     test_size = 0.2, \n","#                                                     random_state = 24)\n","    \n","# vectorizer = CountVectorizer(ngram_range=(1,2))\n","# tweet_matrix = vectorizer.fit_transform(x_train[preprocessing])\n","# test_vector = vectorizer.transform(x_test[preprocessing])"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"09be6346-7ca9-4682-bdff-09b7ac23f970","showTitle":false,"title":""}},"source":["<a id=\"3\"></a>\n","[<font size=\"+2\" ><b>Innovation1: Traditionoal Classifiers vs Bert</b></font><br>](#0)"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"35d32dae-0f61-4c9c-a281-9e976df3a717","showTitle":false,"title":""}},"source":["<a id=\"3.1\"></a>\n","[<font size=\"+1\" ><b>Subjectivity</b></font><br>](#0)"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"7078bd76-160f-47c1-8510-3a2785a8e77d","showTitle":false,"title":""}},"outputs":[],"source":["MAX_LEN=128\n","tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n","\n","def tokenize(data,max_len=MAX_LEN) :\n","    input_ids = []\n","    attention_masks = []\n","    for i in range(len(data)):\n","        encoded = tokenizer.encode_plus(\n","            text=data[i],\n","            add_special_tokens=True,\n","            max_length=MAX_LEN,\n","            padding='max_length',\n","            return_attention_mask=True\n","        )\n","        input_ids.append(encoded['input_ids'])\n","        attention_masks.append(encoded['attention_mask'])\n","    return np.array(input_ids),np.array(attention_masks)"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"1d59ed32-7ce3-4ea6-b5fe-8e84b034fb70","showTitle":false,"title":""}},"outputs":[],"source":["train_input_ids, train_attention_masks = tokenize(list(X_train_sub['cleaned_text']), MAX_LEN)\n","val_input_ids, val_attention_masks = tokenize(list(X_valid_sub['cleaned_text']), MAX_LEN)\n","test_input_ids, test_attention_masks = tokenize(list(X_test_sub['cleaned_text']), MAX_LEN)"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"433fffa2-02c0-43e4-8b7c-6eb4751b1e81","showTitle":false,"title":""}},"outputs":[],"source":["bert_model = TFBertModel.from_pretrained('bert-base-uncased')"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"e446892f-1808-4b03-bc78-5ed483347b56","showTitle":false,"title":""}},"outputs":[],"source":["def create_model(bert_model, max_len=MAX_LEN):\n","    \n","    ##params###\n","    opt = tf.keras.optimizers.Adam(learning_rate=1e-5, decay=1e-7)\n","    loss = tf.keras.losses.CategoricalCrossentropy()\n","    accuracy = tf.keras.metrics.CategoricalAccuracy()\n","\n","\n","    input_ids = tf.keras.Input(shape=(max_len,),dtype='int32')\n","    \n","    attention_masks = tf.keras.Input(shape=(max_len,),dtype='int32')\n","    \n","    embeddings = bert_model([input_ids,attention_masks])[1]\n","    \n","    output = tf.keras.layers.Dense(1, activation=\"softmax\")(embeddings)\n","    \n","    model = tf.keras.models.Model(inputs = [input_ids,attention_masks], outputs = output)\n","    \n","    model.compile(opt, loss=loss, metrics=accuracy)\n","    \n","    return model"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"cb5632d2-d8ec-42fd-b3f7-373f3b557022","showTitle":false,"title":""}},"outputs":[],"source":["model = create_model(bert_model, MAX_LEN)\n","model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"98f14623-29ee-44da-9bca-e436ac15a38b","showTitle":false,"title":""}},"outputs":[],"source":["print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"3622731e-01ad-4635-8e99-f51ae3c6b912","showTitle":false,"title":""}},"outputs":[],"source":["history_bert = model.fit([train_input_ids,train_attention_masks], y_train_sub, validation_data=([val_input_ids,val_attention_masks], y_valid_sub), epochs=4, batch_size=32)"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"8de55df8-468c-460a-9582-61adfaf9c917","showTitle":false,"title":""}},"outputs":[],"source":["y_pred_bert = model.predict([test_input_ids,test_attention_masks])"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"4976101c-d84e-4f55-a0c6-b558e1ac054c","showTitle":false,"title":""}},"outputs":[],"source":["def conf_matrix(y, y_pred, title, labels):\n","    fig, ax =plt.subplots(figsize=(5,5))\n","    ax=sns.heatmap(confusion_matrix(y, y_pred), annot=True, cmap=\"Blues\", fmt='g', cbar=False, annot_kws={\"size\":25})\n","    plt.title(title, fontsize=20)\n","    ax.xaxis.set_ticklabels(labels, fontsize=17) \n","    ax.yaxis.set_ticklabels(labels, fontsize=17)\n","    ax.set_ylabel('Test', fontsize=20)\n","    ax.set_xlabel('Predicted', fontsize=20)\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"a07cfa0f-1614-4340-8bd2-9a943e948364","showTitle":false,"title":""}},"outputs":[],"source":["conf_matrix(y_test_sub, y_pred_bert,'BERT Subjectivity Analysis\\nConfusion Matrix', ['Subjective', 'Opinionated'])"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"27bf1297-97de-469e-97c0-d784e2141802","showTitle":false,"title":""}},"outputs":[],"source":["print('\\tClassification Report for BERT:\\n\\n',classification_report(y_test_sub,y_pred_bert, target_names=['Subjective', 'Opinionated']))"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"e6f8fdd0-fd2b-4665-9b37-372dccf836d3","showTitle":false,"title":""}},"source":["<a id=\"3.1\"></a>\n","[<font size=\"+1\" ><b>Polarity</b></font><br>](#0)"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"52362e17-b192-4099-bc13-caffc0d209a3","showTitle":false,"title":""}},"outputs":[],"source":["polar_train_input_ids, polar_train_attention_masks = tokenize(X_train_polar, MAX_LEN)\n","polar_val_input_ids, polar_val_attention_masks = tokenize(X_valid_polar, MAX_LEN)\n","polar_test_input_ids, polar_test_attention_masks = tokenize(X_test_polar, MAX_LEN)"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"6472e9de-e1a1-4971-896a-ab701ec0ef33","showTitle":false,"title":""}},"outputs":[],"source":["polar_model = create_model(bert_model, MAX_LEN)\n","history_bert_polar = model.fit([polar_train_input_ids,polar_train_attention_masks], y_train_polar, validation_data=([polar_val_input_ids,polar_val_attention_masks], y_valid_polar), epochs=4, batch_size=32)"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"3969cd29-4b4a-4238-9574-be332177c120","showTitle":false,"title":""}},"outputs":[],"source":["y_pred_bert_polar = model.predict([polar_test_input_ids, polar_test_attention_masks])"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"b05827b1-8173-41a5-94fd-32470af8b978","showTitle":false,"title":""}},"outputs":[],"source":["conf_matrix(y_test_polar, y_pred_bert,'BERT Subjectivity Analysis\\nConfusion Matrix', ['Positive', 'Negative'])"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"54c8b80d-6e28-4be1-86b0-17aead7c36ab","showTitle":false,"title":""}},"outputs":[],"source":["print('\\tClassification Report for BERT:\\n\\n',classification_report(y_test_polar,y_pred_bert_polar, target_names=['Positive', 'Negative']))"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"2cc432bf-647f-4fb6-af74-1b95828de44d","showTitle":false,"title":""}},"source":["<a id=\"4\"></a>\n","[<font size=\"+2\" ><b>Innovation2: Two subtask with bert using one-hot-encoding</b></font><br>](#0)"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"68666bef-d3ae-48da-aac0-9e81083dd391","showTitle":false,"title":""}},"outputs":[],"source":[]}],"metadata":{"application/vnd.databricks.v1+notebook":{"dashboards":[],"language":"python","notebookMetadata":{"pythonIndentUnit":4},"notebookName":"Sentiment_Analysis","notebookOrigID":3678102348449288,"widgets":{}},"colab":{"collapsed_sections":[],"provenance":[]},"kernelspec":{"display_name":"Python 3.9.12 ('base')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"8c46ac5aaab4d3c9f2f9be91c3b9b95f2b8ede1f4aae554ca1e34eefcba1468b"}}},"nbformat":4,"nbformat_minor":0}
