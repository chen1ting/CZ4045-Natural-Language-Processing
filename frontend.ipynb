{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a30fdf8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install gradio\n",
    "# Basic Libraries\n",
    "## make sure python version 3.8 and above\n",
    "#!pip install snscrape\n",
    "#!pip install -q -U \"tensorflow-text==2.8.*\"\n",
    "#!pip install tensorflow-text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "646888d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import snscrape.modules.twitter as sntwitter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "import gradio as gr\n",
    "import regex as re\n",
    "import string\n",
    "\n",
    "# preprocessing \n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from langdetect import detect\n",
    "from langdetect import DetectorFactory\n",
    "\n",
    "# bert\n",
    "import tensorflow as tf\n",
    "import tensorflow_text as text\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "77e64661",
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify that model is the same as what we trained"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2524b4",
   "metadata": {},
   "source": [
    "# Sraping\n",
    "- Take search keywords and number of entries\n",
    "- Return list of strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8ff5d1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scrapper for the top games\n",
    "def scraping_game(start, end, game, amount):\n",
    "    tweets_df = {}\n",
    "    \n",
    "    # Creating list to append tweet data to\n",
    "    tweets_list = []\n",
    "\n",
    "    try:\n",
    "        # Using TwitterSearchScraper to scrape data and append tweets to list\n",
    "        for i,tweet in enumerate(sntwitter.TwitterSearchScraper('%s lang:en since:%s until:%s' %(game,start,end)).get_items()):\n",
    "            if i>(amount-1):\n",
    "                break\n",
    "\n",
    "            tweets_list.append([game, tweet.date, tweet.id, tweet.content, tweet.retweetCount, tweet.likeCount, tweet.user.username])\n",
    "\n",
    "    except Exception:\n",
    "        print(Exception)\n",
    "\n",
    "    # Creating a dataframe from the tweets list above\n",
    "    tweets_df[game] = pd.DataFrame(tweets_list, columns=['Game','Datetime', 'TweetId', 'Text', 'RetweetCount', 'LikeCount','Username'])\n",
    "    print(\"Finish Scraping %s for %s\" %(len(tweets_df[game]), game))\n",
    "    \n",
    "    #Concat dict df into one df\n",
    "    # print(tweets_df)\n",
    "    new_df = pd.concat(tweets_df.values(), ignore_index=True)\n",
    "    return new_df['Text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b245b6",
   "metadata": {},
   "source": [
    "# Model\n",
    "- Reloading Model\n",
    "- Preprocessing Inputs: stop after stopwords removal (no stemming/lemmatizing)\n",
    "    - We observe from model training that stemmed/lemmatized text doesn't give SVM models significant better results.So here we are just using cleaned texts\n",
    "- Give predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2690f7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload model\n",
    "BERT_SUB_PATH = './bert_subjectivity_model'\n",
    "BERT_POLAR_PATH = './bert_polarity_model'\n",
    "SVM_SUB_FILE = 'svm_subjectivity_model.sav'\n",
    "SVM_POLAR_FILE = 'svm_polarity_model.sav'\n",
    "\n",
    "BERT_SUB = None\n",
    "BERT_POLAR = None\n",
    "SVM_SUB = None\n",
    "SVM_POLAR = None\n",
    "\n",
    "def reload_model():\n",
    "    BERT_SUB = tf.saved_model.load(BERT_SUB_PATH)\n",
    "    BERT_POLAR = tf.saved_model.load(BERT_POLAR_PATH)\n",
    "    SVM_SUB = pickle.load(open(SVM_SUB_FILE, 'rb'))\n",
    "    SVM_POLAR = pickle.load(open(SVM_POLAR_FILE, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e20e848b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ytchen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ytchen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# set seed\n",
    "DetectorFactory.seed = 0\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "stop = stopwords.words('english')\n",
    "additional_stopwords = [\"'s\",\"...\",\"'ve\",\"``\",\"''\",\"'m\",'--',\"'ll\",\"'d\", 'u', 'b', 'c', 'd', 'x', 'xf', 'f', 'p', 'xb']\n",
    "stop = set(stop + additional_stopwords)\n",
    "\n",
    "def language_detection(x:str):\n",
    "    text = x.split(\" \")\n",
    "    \n",
    "    lang = \"en\"\n",
    "    try:\n",
    "        if len(text) > 50:\n",
    "            lang = detect(\" \".join(text[:50]))\n",
    "        elif len(text) > 0:\n",
    "            lang = detect(\" \".join(text[:len(text)]))\n",
    "    except Exception as e:\n",
    "        all_words = set(text)\n",
    "        try:\n",
    "            lang = detect(\" \".join(all_words))\n",
    "        except Exception as e:\n",
    "            lang = \"unknown\"\n",
    "            pass\n",
    "    return lang\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    text = str(text)\n",
    "    text = re.sub(r'[^a-zA-Z ]+', ' ', text)\n",
    "    text = re.sub(r'http\\S+', ' ', text)\n",
    "    text = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', text)\n",
    "    text = re.sub(r'^RT[\\s]+', '', text)\n",
    "    # text = re.sub(r'pic.twitter\\S+', ' ', text)\n",
    "    text = re.sub(r'#', '', text)\n",
    "    text = text.lower()\n",
    "\n",
    "    return text\n",
    "\n",
    "def decontracted(text):\n",
    "    text = re.sub(r\"won\\'t\", \"will not\", text)\n",
    "    text = re.sub(r\"don't\", \"do not\", text)\n",
    "    text = re.sub(r\"don't\", \"do not\", text)\n",
    "    text = re.sub(r\"can\\'t\", \"can not\", text)\n",
    "    text = re.sub(r\"n\\'t\", \" not\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"it\\'s\", \"it is\", text)\n",
    "    text = re.sub(r\"\\'d\", \" would\", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
    "    text = re.sub(r\"\\'t\", \" not\", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
    "    text = re.sub(r\"\\'m\", \" am\", text)\n",
    "\n",
    "    text = re.sub(r\"n\\'t\", \" not\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"\\'d\", \" would\", text)\n",
    "    text = re.sub(r\"\\'d\", \" would\", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
    "    text = re.sub(r\"\\'t\", \" not\", text)\n",
    "    text = re.sub(r\"\\'t\", \" not\", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
    "    text = re.sub(r\"\\'m\", \" am\", text)\n",
    "    text = re.sub(r\"\\'m\", \" am\", text)\n",
    "    text = re.sub(r\"\\“\", \"\", text)\n",
    "    text = re.sub(r\"\\”\", \"\", text)\n",
    "    text = re.sub(r\"\\…\", \"\", text)\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_punc(tweet):\n",
    "    tweet =  tweet.translate(str.maketrans('', '', string.punctuation))\n",
    "    tweet = ' '.join([word for word in tweet.split()])\n",
    "    tweet = tweet.lower()\n",
    "    \n",
    "    return tweet\n",
    "\n",
    "\n",
    "def remove_stopwords(x):\n",
    "    x = word_tokenize(x)\n",
    "    store_words = ''\n",
    "    \n",
    "    for i in x:\n",
    "        if i not in stop:\n",
    "            store_words += i + ' '\n",
    "            \n",
    "    return store_words\n",
    "\n",
    "\n",
    "def pre_process(tweet):\n",
    "    if language_detection(tweet) != 'en':\n",
    "        return None         # suggesting not english language and cannot give predictions\n",
    "    return remove_stopwords(remove_punc(decontracted(clean_text(tweet))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1f1eee43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_predict(cleaned_text:list):\n",
    "    result_bert_sub = BERT_SUB.predict(cleaned_text)\n",
    "    result_bert_polar = BERT_POLAR.predict(cleaned_text)\n",
    "    return result_bert_sub, result_bert_polar\n",
    "\n",
    "def svm_predict(cleaned_text:list):\n",
    "    result_svm_sub = SVM_SUB.predict(cleaned_text)\n",
    "    result_svm_polar = SVM_POLAR.predict(cleaned_text)\n",
    "    return result_svm_sub, result_svm_polar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959503dc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#function for analyzing overall sentiment\n",
    "def analyze_sentiment(sentiment):\n",
    "    if sentiment < 0.25:\n",
    "        overall_sentiment = \"very negative\"\n",
    "    elif sentiment < 0.5:\n",
    "        overall_sentiment = \"negative\"\n",
    "    elif sentiment < 0.75:\n",
    "        overall_sentiment = \"positive\"\n",
    "    else:\n",
    "        overall_sentiment = \"very positive\"\n",
    "    return overall_sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "388ba99b",
   "metadata": {},
   "source": [
    "# Main Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "33fbd503",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init():\n",
    "    reload_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd7d5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main NLP program\n",
    "def nlp(game_title, scrap_no, activate_scrape, algorithm_choice):\n",
    "    \n",
    "    #insert function to scrape twitter for game title here\n",
    "    scrapped_tweets_raw = scraping_game(\"2022-01-01\", \"2022-11-04\", game_title, scrap_no)\n",
    "    #insert preprocessing function here\n",
    "    \n",
    "    cleaned_text = [pre_process(text) for text in scrapped_tweets_raw]\n",
    "    cleaned_text = [len(text.split())>1 for text in cleaned_text]\n",
    "    \n",
    "    #Reviewing overall sentiment\n",
    "    sentiment = 0.22\n",
    "    \n",
    "    #Check which models to run \n",
    "    #NOTE INSERT PREDICTOR MODEL AS LABELED\n",
    "    if 'Bert' in algorithm_choice:\n",
    "        #Insert Bert model predictor here\n",
    "        result_bert_sub, result_bert_polar = bert_predict(cleaned_text) # return individual results\n",
    "        bert_sentiment_result = f\"BERT returned {len(result_bert_sub)} results, average score = {np.mean(result_bert_sub)}\"\n",
    "        bert_polarity_result = f\"BERT model returned {len(result_bert_polar)} results, average score = {np.mean(result_bert_polar)}\"\n",
    "    else:\n",
    "        bert_sentiment_result, bert_polarity_result = \"Bert model not being run\"\n",
    "        \n",
    "    #Repeat for SVM model\n",
    "    if 'SVM' in algorithm_choice:\n",
    "        #Insert SVM model predictor here\n",
    "        result_svm_sub, result_svm_polar = svm_predict(cleaned_text)\n",
    "        SVM_sentiment_result = f\"SVM returned {len(result_svm_sub)} results, average score = {np.mean(result_svm_sub)}\"\n",
    "        SVM_polarity_result = f\"SVM model returned {len(result_svm_polar)} results, average score = {np.mean(result_svm_polar)}\"\n",
    "    else:\n",
    "        SVM_sentiment_result, SVM_polarity_result = \"SVM model not being run\"\n",
    "    \n",
    "    #if scrape checkbox is marked\n",
    "    if activate_scrape:\n",
    "        return {bert_sentiment : game_title + bert_sentiment_result,\n",
    "                bert_polarity : bert_polarity_result,\n",
    "                svm_sentiment : SVM_sentiment_result,\n",
    "                svm_polarity : SVM_polarity_result,\n",
    "                scraped_tweets : scrapped_tweets_raw}\n",
    "    else:\n",
    "        return {bert_sentiment : game_title + \" \" + bert_sentiment_result,\n",
    "                bert_polarity : bert_polarity_result,\n",
    "                svm_sentiment : SVM_sentiment_result,\n",
    "                svm_polarity : SVM_polarity_result,}\n",
    "\n",
    "##EDIT TO INCLUDE EXCEPTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "175ace8e",
   "metadata": {},
   "source": [
    "# Gradio Frontend code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6355ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "with gr.Blocks() as demo:\n",
    "    with gr.Row():\n",
    "        #First Column\n",
    "        with gr.Column(scale=1):\n",
    "            game_title = gr.Textbox(label = \"Game Title\")\n",
    "            \n",
    "            #amount of tweets to scrape\n",
    "            scrap_no = gr.Slider(0,1000, label = \"Amount of tweets to scrape\")\n",
    "            \n",
    "            #Choose to display scraped text\n",
    "            activate_scrape = gr.Checkbox(label = \"Show scraped data?\")\n",
    "            \n",
    "            #Choice of algorithm\n",
    "            algorithm_choice = gr.CheckboxGroup(choices = [\"Bert\", \"SVM\"]),\n",
    "            \n",
    "            submit_button = gr.Button(\"Submit\")\n",
    "            \n",
    "            #Displays scrapped tweets if option is selected\n",
    "            scraped_tweets = gr.Textbox(label = \"Scraped Data\")\n",
    "            \n",
    "        #Second Column displays all model results\n",
    "        with gr.Column(scale=4):\n",
    "            bert_sentiment = gr.Textbox(label = \"Bert Sentiment\")\n",
    "            bert_polarity = gr.Textbox(label = \"Bert Polarity\")\n",
    "            svm_sentiment = gr.Textbox(label = \"SVM Sentiment\")\n",
    "            svm_polarity = gr.Textbox(label = \"SVM Polarity\")\n",
    "    \n",
    "    #Button to run nlp function\n",
    "    submit_button.click(nlp, \n",
    "                        inputs=[game_title,scrap_no,activate_scrape,algorithm_choice[0]], \n",
    "                        outputs=[bert_sentiment,\n",
    "                                bert_polarity,\n",
    "                                svm_sentiment,\n",
    "                                svm_polarity, \n",
    "                                scraped_tweets]\n",
    "                       )\n",
    "    \n",
    "\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f783431",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "2d40b353eadc8cbb987c2ca5c7db5aac5278416995cd628ee8f7b8b865cb97db"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
