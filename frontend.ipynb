{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a30fdf8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install gradio\n",
    "# Basic Libraries\n",
    "## make sure python version 3.8 and above\n",
    "#!pip install snscrape\n",
    "#!pip install -q -U \"tensorflow-text==2.8.*\"\n",
    "#!pip install tensorflow-text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "646888d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import snscrape.modules.twitter as sntwitter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "import gradio as gr\n",
    "import regex as re\n",
    "import string\n",
    "from time import time\n",
    "\n",
    "# preprocessing \n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from langdetect import detect\n",
    "from langdetect import DetectorFactory\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "# bert\n",
    "import tensorflow as tf\n",
    "import tensorflow_text as text\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2524b4",
   "metadata": {},
   "source": [
    "# Sraping\n",
    "- Take search keywords and number of entries\n",
    "- Return list of strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ff5d1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scrapper for the top games\n",
    "def scraping_game(start, end, game, amount):\n",
    "    tweets_df = {}\n",
    "    \n",
    "    # Creating list to append tweet data to\n",
    "    tweets_list = []\n",
    "\n",
    "    try:\n",
    "        # Using TwitterSearchScraper to scrape data and append tweets to list\n",
    "        for i,tweet in enumerate(sntwitter.TwitterSearchScraper('%s lang:en since:%s until:%s' %(game,start,end)).get_items()):\n",
    "            if i>(amount-1):\n",
    "                break\n",
    "\n",
    "            tweets_list.append([game, tweet.date, tweet.id, tweet.content, tweet.retweetCount, tweet.likeCount, tweet.user.username])\n",
    "\n",
    "    except Exception:\n",
    "        print(Exception)\n",
    "\n",
    "    # Creating a dataframe from the tweets list above\n",
    "    tweets_df[game] = pd.DataFrame(tweets_list, columns=['Game','Datetime', 'TweetId', 'Text', 'RetweetCount', 'LikeCount','Username'])\n",
    "    print(\"Finish Scraping %s for %s\" %(len(tweets_df[game]), game))\n",
    "    \n",
    "    #Concat dict df into one df\n",
    "    # print(tweets_df)\n",
    "    new_df = pd.concat(tweets_df.values(), ignore_index=True)\n",
    "    return new_df['Text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d33628a",
   "metadata": {},
   "source": [
    "# Helpers\n",
    "- Timer\n",
    "- Sentiment Analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1221d5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def timer_func(func):\n",
    "    # This function shows the execution time of \n",
    "    # the function object passed\n",
    "    def wrap_func(*args, **kwargs):\n",
    "        t1 = time()\n",
    "        result = func(*args, **kwargs)\n",
    "        t2 = time()\n",
    "        print(f'{func.__name__!r} executed in {(t2-t1):.4f}s')\n",
    "        return result\n",
    "    return wrap_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef62ca28",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#function for analyzing overall sentiment\n",
    "def analyze_sentiment(sentiment, neg_hint:str, pos_hint:str):\n",
    "    if sentiment < 0.25:\n",
    "        overall_sentiment = f\"very {neg_hint}\"\n",
    "    elif sentiment < 0.5:\n",
    "        overall_sentiment = f\"{neg_hint}\"\n",
    "    elif sentiment < 0.75:\n",
    "        overall_sentiment = f\"{pos_hint}\"\n",
    "    else:\n",
    "        overall_sentiment = f\"very {pos_hint}\"\n",
    "    return overall_sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b245b6",
   "metadata": {},
   "source": [
    "# Model\n",
    "- Reloading Model\n",
    "- Preprocessing Inputs: stop after stopwords removal (no stemming/lemmatizing)\n",
    "    - We observe from model training that stemmed/lemmatized text doesn't give SVM models significant better results.So here we are just using cleaned texts\n",
    "- Give predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2690f7e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\miniconda3\\lib\\site-packages\\sklearn\\base.py:329: UserWarning: Trying to unpickle estimator SVC from version 1.0.2 when using version 1.1.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# reload model\n",
    "BERT_SUB = tf.saved_model.load('./bert_subjectivity_model')\n",
    "BERT_POLAR = tf.saved_model.load('./bert_polarity_model')\n",
    "SVM_SUB = pickle.load(open('svm_subjectivity_model.sav', 'rb'))\n",
    "SVM_POLAR = pickle.load(open('svm_polarity_model.sav', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1057e725",
   "metadata": {},
   "outputs": [],
   "source": [
    "@timer_func\n",
    "def bert_predict(cleaned_text:list):\n",
    "    result_bert_sub = BERT_SUB(cleaned_text)\n",
    "    result_bert_polar = BERT_POLAR(cleaned_text)\n",
    "    return np.array(tf.sigmoid(result_bert_sub))[:,0], np.array(tf.sigmoid(result_bert_polar))[:,0]\n",
    "\n",
    "@timer_func\n",
    "def svm_predict(cleaned_text:list):\n",
    "    cv = CountVectorizer(ngram_range=(1,2), max_features=500).fit_transform(cleaned_text)\n",
    "    tfidf_texts = TfidfTransformer(use_idf=True).fit_transform(cv)\n",
    "    result_svm_sub = SVM_SUB.predict(tfidf_texts)\n",
    "    result_svm_polar = SVM_POLAR.predict(tfidf_texts)\n",
    "    return result_svm_sub, result_svm_polar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e20e848b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# set seed\n",
    "DetectorFactory.seed = 0\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "stop = stopwords.words('english')\n",
    "additional_stopwords = [\"'s\",\"...\",\"'ve\",\"``\",\"''\",\"'m\",'--',\"'ll\",\"'d\", 'u', 'b', 'c', 'd', 'x', 'xf', 'f', 'p', 'xb']\n",
    "stop = set(stop + additional_stopwords)\n",
    "\n",
    "def language_detection(x:str):\n",
    "    text = x.split(\" \")\n",
    "    \n",
    "    lang = \"en\"\n",
    "    try:\n",
    "        if len(text) > 50:\n",
    "            lang = detect(\" \".join(text[:50]))\n",
    "        elif len(text) > 0:\n",
    "            lang = detect(\" \".join(text[:len(text)]))\n",
    "    except Exception as e:\n",
    "        all_words = set(text)\n",
    "        try:\n",
    "            lang = detect(\" \".join(all_words))\n",
    "        except Exception as e:\n",
    "            lang = \"unknown\"\n",
    "            pass\n",
    "    return lang\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    text = str(text)\n",
    "    text = re.sub(r'[^a-zA-Z ]+', ' ', text)\n",
    "    text = re.sub(r'http\\S+', ' ', text)\n",
    "    text = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', text)\n",
    "    text = re.sub(r'^RT[\\s]+', '', text)\n",
    "    # text = re.sub(r'pic.twitter\\S+', ' ', text)\n",
    "    text = re.sub(r'#', '', text)\n",
    "    text = text.lower()\n",
    "\n",
    "    return text\n",
    "\n",
    "def decontracted(text):\n",
    "    text = re.sub(r\"won\\'t\", \"will not\", text)\n",
    "    text = re.sub(r\"don't\", \"do not\", text)\n",
    "    text = re.sub(r\"don't\", \"do not\", text)\n",
    "    text = re.sub(r\"can\\'t\", \"can not\", text)\n",
    "    text = re.sub(r\"n\\'t\", \" not\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"it\\'s\", \"it is\", text)\n",
    "    text = re.sub(r\"\\'d\", \" would\", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
    "    text = re.sub(r\"\\'t\", \" not\", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
    "    text = re.sub(r\"\\'m\", \" am\", text)\n",
    "\n",
    "    text = re.sub(r\"n\\'t\", \" not\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"\\'d\", \" would\", text)\n",
    "    text = re.sub(r\"\\'d\", \" would\", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
    "    text = re.sub(r\"\\'t\", \" not\", text)\n",
    "    text = re.sub(r\"\\'t\", \" not\", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
    "    text = re.sub(r\"\\'m\", \" am\", text)\n",
    "    text = re.sub(r\"\\'m\", \" am\", text)\n",
    "    text = re.sub(r\"\\“\", \"\", text)\n",
    "    text = re.sub(r\"\\”\", \"\", text)\n",
    "    text = re.sub(r\"\\…\", \"\", text)\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_punc(tweet):\n",
    "    tweet =  tweet.translate(str.maketrans('', '', string.punctuation))\n",
    "    tweet = ' '.join([word for word in tweet.split()])\n",
    "    tweet = tweet.lower()\n",
    "    \n",
    "    return tweet\n",
    "\n",
    "\n",
    "def remove_stopwords(x):\n",
    "    x = word_tokenize(x)\n",
    "    store_words = ''\n",
    "    \n",
    "    for i in x:\n",
    "        if i not in stop:\n",
    "            store_words += i + ' '\n",
    "            \n",
    "    return store_words\n",
    "\n",
    "\n",
    "def pre_process(tweet):\n",
    "    if language_detection(tweet) != 'en':\n",
    "        return None         # suggesting not english language and cannot give predictions\n",
    "    return remove_stopwords(remove_punc(decontracted(clean_text(tweet))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "388ba99b",
   "metadata": {},
   "source": [
    "# Main Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3bd7d5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main NLP program\n",
    "def nlp(game_title, scrap_no, activate_scrape, algorithm_choice):\n",
    "    \n",
    "    #insert function to scrape twitter for game title here\n",
    "    scrapped_tweets_raw = scraping_game(\"2022-01-01\", \"2022-11-04\", game_title, scrap_no)\n",
    "    #insert preprocessing function here\n",
    "    print(\"Preprocessing texts now...\")\n",
    "    cleaned_text = [pre_process(text) for text in scrapped_tweets_raw]\n",
    "    cleaned_text = [text for text in cleaned_text if text and\n",
    "                    len(text.split())>1]\n",
    "    \n",
    "    #Check which models to run \n",
    "    #NOTE INSERT PREDICTOR MODEL AS LABELED\n",
    "    if 'Bert' in algorithm_choice:\n",
    "        print(\"Running Bert now...\")\n",
    "        #Insert Bert model predictor here\n",
    "        result_bert_sub, result_bert_polar = bert_predict(cleaned_text) # return individual results\n",
    "        bert_sub_avg = np.mean(result_bert_sub)\n",
    "        bert_polar_avg = np.mean(result_bert_polar)\n",
    "        bert_sentiment_result = f\"average score = {bert_sub_avg}, {analyze_sentiment(bert_sub_avg, 'neutral', 'subjective')}\"\n",
    "        bert_polarity_result = f\"average score = {bert_polar_avg}, {analyze_sentiment(bert_polar_avg, 'negative', 'positive')}\"\n",
    "    else:\n",
    "        bert_sentiment_result, bert_polarity_result = \"Bert model not being run\"\n",
    "    \n",
    "    \n",
    "    #Repeat for SVM model\n",
    "    if 'SVM' in algorithm_choice:\n",
    "        print(\"Running SVM now...\")\n",
    "        #Insert SVM model predictor here\n",
    "        result_svm_sub, result_svm_polar = svm_predict(cleaned_text)\n",
    "        SVM_sub_avg = np.mean(result_svm_sub)\n",
    "        SVM_polar_avg = np.mean(result_svm_polar)\n",
    "        SVM_sentiment_result = f\"average score = {SVM_sub_avg}, {analyze_sentiment(SVM_sub_avg, 'neutral', 'subjective')}\"\n",
    "        SVM_polarity_result = f\"average score = {SVM_polar_avg}, {analyze_sentiment(SVM_polar_avg, 'negative', 'positive')}\"\n",
    "    else:\n",
    "        SVM_sentiment_result, SVM_polarity_result = \"SVM model not being run\"\n",
    "        \n",
    "    print(\"Returning message\")\n",
    "    #if scrape checkbox is marked\n",
    "    if activate_scrape:\n",
    "        result_df = pd.DataFrame({'Cleaned Text':cleaned_text,\n",
    "                                               'Bert Subjectivity':result_bert_sub,\n",
    "                                               'Bert Polarity':result_bert_polar,\n",
    "                                               'SVM Subjectivity':result_svm_sub,\n",
    "                                               'SVM Polarity':result_svm_polar})\n",
    "        return {bert_sentiment : game_title + bert_sentiment_result,\n",
    "                bert_polarity : bert_polarity_result,\n",
    "                svm_sentiment : SVM_sentiment_result,\n",
    "                svm_polarity : SVM_polarity_result, \n",
    "                scraped_tweets: result_df}\n",
    "    else:\n",
    "        return {bert_sentiment : game_title + \" \" + bert_sentiment_result,\n",
    "                bert_polarity : bert_polarity_result,\n",
    "                svm_sentiment : SVM_sentiment_result,\n",
    "                svm_polarity : SVM_polarity_result}\n",
    "\n",
    "##EDIT TO INCLUDE EXCEPTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "175ace8e",
   "metadata": {},
   "source": [
    "# Gradio Frontend code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4e6355ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Thanks for being a Gradio user! If you have questions or feedback, please join our Discord server and chat with us: https://discord.gg/feTf9x3ZSB\n",
      "Running on local URL:  http://127.0.0.1:7872\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7872/\" width=\"900\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(<gradio.routes.App at 0x22416d5e820>, 'http://127.0.0.1:7872/', None)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Scraping 200 for pubg\n",
      "Preprocessing texts now...\n",
      "Running Bert now...\n",
      "'bert_predict' executed in 15.4881s\n",
      "Running SVM now...\n",
      "'svm_predict' executed in 0.3030s\n",
      "Returning message\n",
      "Finish Scraping 200 for pubg\n",
      "Preprocessing texts now...\n",
      "Running Bert now...\n",
      "'bert_predict' executed in 13.5410s\n",
      "Running SVM now...\n",
      "'svm_predict' executed in 0.2000s\n",
      "Returning message\n"
     ]
    }
   ],
   "source": [
    "with gr.Blocks() as demo:\n",
    "    with gr.Row():\n",
    "        #First Column\n",
    "        with gr.Column(scale=1):\n",
    "            game_title = gr.Textbox(label = \"Game Title\")\n",
    "            \n",
    "            #amount of tweets to scrape\n",
    "            scrap_no = gr.Slider(200,1000, label = \"Amount of tweets to scrape\") #SVM requires more thajn 500 features\n",
    "            \n",
    "            #Choose to display scraped text\n",
    "            activate_scrape = gr.Checkbox(label = \"Show scraped data?\")\n",
    "            \n",
    "            #Choice of algorithm\n",
    "            algorithm_choice = gr.CheckboxGroup(choices = [\"Bert\", \"SVM\"]),\n",
    "            \n",
    "            submit_button = gr.Button(\"Submit\")\n",
    "            \n",
    "        #Second Column displays all model results\n",
    "        with gr.Column(scale=4):\n",
    "            bert_sentiment = gr.Textbox(label = \"Bert Sentiment\")\n",
    "            bert_polarity = gr.Textbox(label = \"Bert Polarity\")\n",
    "            svm_sentiment = gr.Textbox(label = \"SVM Sentiment\")\n",
    "            svm_polarity = gr.Textbox(label = \"SVM Polarity\")\n",
    "            \n",
    "    with gr.Row():       \n",
    "        #Displays scrapped tweets if option is selected\n",
    "        scraped_tweets = gr.DataFrame(label = \"Scraped Data\", headers=['Cleaned Text',\n",
    "                                               'Bert Subjectivity',\n",
    "                                               'Bert Polarity',\n",
    "                                               'SVM Subjectivity',\n",
    "                                               'SVM Polarity'])\n",
    "    #Button to run nlp function\n",
    "    submit_button.click(nlp, \n",
    "                        inputs=[game_title,scrap_no,activate_scrape,algorithm_choice[0]], \n",
    "                        outputs=[bert_sentiment,\n",
    "                                 bert_polarity,\n",
    "                                 svm_sentiment,\n",
    "                                 svm_polarity,\n",
    "                                 scraped_tweets]\n",
    "                       )\n",
    "\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a406a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50c670b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "2d40b353eadc8cbb987c2ca5c7db5aac5278416995cd628ee8f7b8b865cb97db"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
