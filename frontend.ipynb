{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30fdf8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install gradio\n",
    "# Basic Libraries\n",
    "## make sure python version 3.8 and above\n",
    "#!pip install snscrape\n",
    "#!pip install -q -U \"tensorflow-text==2.8.*\"\n",
    "#!pip install tensorflow-text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646888d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import snscrape.modules.twitter as sntwitter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "import gradio as gr\n",
    "import regex as re\n",
    "import string\n",
    "from time import time\n",
    "\n",
    "# preprocessing \n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from langdetect import detect\n",
    "from langdetect import DetectorFactory\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "# bert\n",
    "import tensorflow as tf\n",
    "import tensorflow_text as text\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2524b4",
   "metadata": {},
   "source": [
    "# Sraping\n",
    "- Take search keywords and number of entries\n",
    "- Return list of strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff5d1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scrapper for the top games\n",
    "def scraping_game(start, end, game, amount):\n",
    "    tweets_df = {}\n",
    "    \n",
    "    # Creating list to append tweet data to\n",
    "    tweets_list = []\n",
    "\n",
    "    try:\n",
    "        # Using TwitterSearchScraper to scrape data and append tweets to list\n",
    "        for i,tweet in enumerate(sntwitter.TwitterSearchScraper('%s lang:en since:%s until:%s' %(game,start,end)).get_items()):\n",
    "            if i>(amount-1):\n",
    "                break\n",
    "\n",
    "            tweets_list.append([game, tweet.date, tweet.id, tweet.content, tweet.retweetCount, tweet.likeCount, tweet.user.username])\n",
    "\n",
    "    except Exception:\n",
    "        print(Exception)\n",
    "\n",
    "    # Creating a dataframe from the tweets list above\n",
    "    tweets_df[game] = pd.DataFrame(tweets_list, columns=['Game','Datetime', 'TweetId', 'Text', 'RetweetCount', 'LikeCount','Username'])\n",
    "    print(\"Finish Scraping %s for %s\" %(len(tweets_df[game]), game))\n",
    "    \n",
    "    #Concat dict df into one df\n",
    "    # print(tweets_df)\n",
    "    new_df = pd.concat(tweets_df.values(), ignore_index=True)\n",
    "    return new_df['Text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d33628a",
   "metadata": {},
   "source": [
    "# Helpers\n",
    "- Timer\n",
    "- Sentiment Analyzer\n",
    "- Map Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1221d5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def timer_func(func):\n",
    "    # This function shows the execution time of \n",
    "    # the function object passed\n",
    "    def wrap_func(*args, **kwargs):\n",
    "        t1 = time()\n",
    "        arr1, arr2 = func(*args, **kwargs)\n",
    "        t2 = time()\n",
    "        print(f'{func.__name__!r} executed in {(t2-t1):.4f}s')\n",
    "        return arr1, arr2, (t2-t1)\n",
    "    return wrap_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef62ca28",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#function for analyzing overall sentiment\n",
    "def analyze_sentiment(results, neg_hint:str, pos_hint:str):\n",
    "    sentiment = np.mean(results)\n",
    "    if sentiment < 0.25:\n",
    "        overall_sentiment = f\"very {neg_hint}\"\n",
    "    elif sentiment < 0.5:\n",
    "        overall_sentiment = f\"{neg_hint}\"\n",
    "    elif sentiment < 0.75:\n",
    "        overall_sentiment = f\"{pos_hint}\"\n",
    "    else:\n",
    "        overall_sentiment = f\"very {pos_hint}\"\n",
    "    return f\"\"\"{pos_hint} tweets = {int(np.sum(results))}, total tweets = {len(results)}\n",
    "    average score = {sentiment:.4}, generally {overall_sentiment}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e135716c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_map(results, neg_hint:str, pos_hint:str):\n",
    "    hints = []\n",
    "    for result in results:\n",
    "        if result == 0:\n",
    "            hints.append(neg_hint)\n",
    "        else:\n",
    "            hints.append(pos_hint)\n",
    "    return hints"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b245b6",
   "metadata": {},
   "source": [
    "# Model\n",
    "- Reloading Model\n",
    "- Preprocessing Inputs: stop after stopwords removal (no stemming/lemmatizing)\n",
    "    - We observe from model training that stemmed/lemmatized text doesn't give SVM models significant better results.So here we are just using cleaned texts\n",
    "- Give predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2690f7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload model\n",
    "BERT_SUB = tf.saved_model.load('./bert_subjectivity_model')\n",
    "BERT_POLAR = tf.saved_model.load('./bert_polarity_model')\n",
    "SVM_SUB = pickle.load(open('svm_subjectivity_model.sav', 'rb'))\n",
    "SVM_POLAR = pickle.load(open('svm_polarity_model.sav', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1057e725",
   "metadata": {},
   "outputs": [],
   "source": [
    "@timer_func\n",
    "def bert_predict(cleaned_text:list):\n",
    "    result_bert_sub = BERT_SUB(cleaned_text)\n",
    "    result_bert_polar = BERT_POLAR(cleaned_text)\n",
    "    return np.round(np.array(tf.sigmoid(result_bert_sub))[:,0]), np.round(np.array(tf.sigmoid(result_bert_polar))[:,0])\n",
    "\n",
    "@timer_func\n",
    "def svm_predict(cleaned_text:list):\n",
    "    cv = CountVectorizer(ngram_range=(1,2), max_features=500).fit_transform(cleaned_text)\n",
    "    tfidf_texts = TfidfTransformer(use_idf=True).fit_transform(cv)\n",
    "    result_svm_sub = SVM_SUB.predict(tfidf_texts)\n",
    "    result_svm_polar = SVM_POLAR.predict(tfidf_texts)\n",
    "    return result_svm_sub, result_svm_polar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20e848b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seed\n",
    "DetectorFactory.seed = 0\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "stop = stopwords.words('english')\n",
    "additional_stopwords = [\"'s\",\"...\",\"'ve\",\"``\",\"''\",\"'m\",'--',\"'ll\",\"'d\", 'u', 'b', 'c', 'd', 'x', 'xf', 'f', 'p', 'xb']\n",
    "stop = set(stop + additional_stopwords)\n",
    "\n",
    "def language_detection(x:str):\n",
    "    text = x.split(\" \")\n",
    "    \n",
    "    lang = \"en\"\n",
    "    try:\n",
    "        if len(text) > 50:\n",
    "            lang = detect(\" \".join(text[:50]))\n",
    "        elif len(text) > 0:\n",
    "            lang = detect(\" \".join(text[:len(text)]))\n",
    "    except Exception as e:\n",
    "        all_words = set(text)\n",
    "        try:\n",
    "            lang = detect(\" \".join(all_words))\n",
    "        except Exception as e:\n",
    "            lang = \"unknown\"\n",
    "            pass\n",
    "    return lang\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    text = str(text)\n",
    "    text = re.sub(r'[^a-zA-Z ]+', ' ', text)\n",
    "    text = re.sub(r'http\\S+', ' ', text)\n",
    "    text = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', text)\n",
    "    text = re.sub(r'^RT[\\s]+', '', text)\n",
    "    # text = re.sub(r'pic.twitter\\S+', ' ', text)\n",
    "    text = re.sub(r'#', '', text)\n",
    "    text = text.lower()\n",
    "\n",
    "    return text\n",
    "\n",
    "def decontracted(text):\n",
    "    text = re.sub(r\"won\\'t\", \"will not\", text)\n",
    "    text = re.sub(r\"don't\", \"do not\", text)\n",
    "    text = re.sub(r\"don't\", \"do not\", text)\n",
    "    text = re.sub(r\"can\\'t\", \"can not\", text)\n",
    "    text = re.sub(r\"n\\'t\", \" not\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"it\\'s\", \"it is\", text)\n",
    "    text = re.sub(r\"\\'d\", \" would\", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
    "    text = re.sub(r\"\\'t\", \" not\", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
    "    text = re.sub(r\"\\'m\", \" am\", text)\n",
    "\n",
    "    text = re.sub(r\"n\\'t\", \" not\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"\\'d\", \" would\", text)\n",
    "    text = re.sub(r\"\\'d\", \" would\", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
    "    text = re.sub(r\"\\'t\", \" not\", text)\n",
    "    text = re.sub(r\"\\'t\", \" not\", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
    "    text = re.sub(r\"\\'m\", \" am\", text)\n",
    "    text = re.sub(r\"\\'m\", \" am\", text)\n",
    "    text = re.sub(r\"\\“\", \"\", text)\n",
    "    text = re.sub(r\"\\”\", \"\", text)\n",
    "    text = re.sub(r\"\\…\", \"\", text)\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_punc(tweet):\n",
    "    tweet =  tweet.translate(str.maketrans('', '', string.punctuation))\n",
    "    tweet = ' '.join([word for word in tweet.split()])\n",
    "    tweet = tweet.lower()\n",
    "    \n",
    "    return tweet\n",
    "\n",
    "\n",
    "def remove_stopwords(x):\n",
    "    x = word_tokenize(x)\n",
    "    store_words = ''\n",
    "    \n",
    "    for i in x:\n",
    "        if i not in stop:\n",
    "            store_words += i + ' '\n",
    "            \n",
    "    return store_words\n",
    "\n",
    "\n",
    "def pre_process(tweet):\n",
    "    if language_detection(tweet) != 'en':\n",
    "        return None         # suggesting not english language and cannot give predictions\n",
    "    return remove_stopwords(remove_punc(decontracted(clean_text(tweet))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42067a61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "388ba99b",
   "metadata": {},
   "source": [
    "# Main Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd7d5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main NLP program\n",
    "def nlp(game_title, scrap_no, activate_scrape, algorithm_choice):\n",
    "    \n",
    "    results = {}\n",
    "    scrapped_tweets_raw = scraping_game(\"2022-01-01\", \"2022-11-04\", game_title, scrap_no)\n",
    "    #insert preprocessing function here\n",
    "    print(\"Preprocessing texts now...\")\n",
    "    preprocessing_result = [pre_process(text) for text in scrapped_tweets_raw]\n",
    "    cleaned_text, original_text = [],[]\n",
    "    for i in range(len(scrapped_tweets_raw)):\n",
    "        if preprocessing_result[i] and len(preprocessing_result[i].split())>1:\n",
    "            cleaned_text.append(preprocessing_result[i])\n",
    "            original_text.append(scrapped_tweets_raw[i])\n",
    "    results['Original Text'] = original_text\n",
    "    results['Cleaned Text'] = cleaned_text\n",
    "    \n",
    "    #Check which models to run \n",
    "    #NOTE INSERT PREDICTOR MODEL AS LABELED\n",
    "    if 'Bert' in algorithm_choice:\n",
    "        print(\"Running Bert now...\")\n",
    "        #Insert Bert model predictor here\n",
    "        result_bert_sub, result_bert_polar, bert_exe_time = bert_predict(cleaned_text) # return individual results\n",
    "        results['Bert Subjectivity'] = sentiment_map(result_bert_sub, 'neutral', 'subjective')\n",
    "        results['Bert Polarity'] = sentiment_map(result_bert_polar,'negative', 'positive')\n",
    "        \n",
    "        bert_sentiment_result = analyze_sentiment(result_bert_sub, 'neutral', 'subjective')\n",
    "        bert_polarity_result = analyze_sentiment(result_bert_polar, 'negative', 'positive')\n",
    "        \n",
    "    else:\n",
    "        bert_sentiment_result, bert_polarity_result = \"Bert model not being run\",\"Bert model not being run\"\n",
    "    \n",
    "    \n",
    "    #Repeat for SVM model\n",
    "    if 'SVM' in algorithm_choice:\n",
    "        print(\"Running SVM now...\")\n",
    "        #Insert SVM model predictor here\n",
    "        result_svm_sub, result_svm_polar, svm_exe_time = svm_predict(cleaned_text)\n",
    "        results['SVM Subjectivity'] = sentiment_map(result_svm_sub, 'neutral', 'subjective')\n",
    "        results['SVM Polarity'] = sentiment_map(result_svm_polar,'negative', 'positive')\n",
    "        \n",
    "        SVM_sentiment_result = analyze_sentiment(result_svm_sub, 'neutral', 'subjective')\n",
    "        SVM_polarity_result = analyze_sentiment(result_svm_polar, 'negative', 'positive')\n",
    "    else:\n",
    "        SVM_sentiment_result, SVM_polarity_result = \"SVM model not being run\",\"SVM model not being run\"\n",
    "        \n",
    "    print(\"Returning message\")\n",
    "    #if scrape checkbox is marked\n",
    "    if activate_scrape:\n",
    "        return {bert_sentiment : bert_sentiment_result,\n",
    "                bert_polarity : bert_polarity_result,\n",
    "                svm_sentiment : SVM_sentiment_result,\n",
    "                svm_polarity : SVM_polarity_result, \n",
    "                scraped_tweets: pd.DataFrame(results)}\n",
    "    else:\n",
    "        return {bert_sentiment : game_title + \" \" + bert_sentiment_result,\n",
    "                bert_polarity : bert_polarity_result,\n",
    "                svm_sentiment : SVM_sentiment_result,\n",
    "                svm_polarity : SVM_polarity_result}\n",
    "\n",
    "##EDIT TO INCLUDE EXCEPTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "175ace8e",
   "metadata": {},
   "source": [
    "# Gradio Frontend code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6355ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "with gr.Blocks() as demo:\n",
    "    with gr.Row():\n",
    "        #First Column\n",
    "        with gr.Column(scale=1):\n",
    "            game_title = gr.Textbox(label = \"Game Title\")\n",
    "            \n",
    "            #amount of tweets to scrape\n",
    "            scrap_no = gr.Slider(200,1000, label = \"Amount of tweets to scrape\") #SVM requires more thajn 500 features\n",
    "            \n",
    "            #Choose to display scraped text\n",
    "            activate_scrape = gr.Checkbox(label = \"Show scraped data?\")\n",
    "            \n",
    "            #Choice of algorithm\n",
    "            algorithm_choice = gr.CheckboxGroup(choices = [\"Bert\", \"SVM\"]),\n",
    "            \n",
    "            submit_button = gr.Button(\"Submit\")\n",
    "            \n",
    "        #Second Column displays all model results\n",
    "        with gr.Column(scale=4):\n",
    "            bert_sentiment = gr.Textbox(label = \"Bert Subjectivity\")\n",
    "            bert_polarity = gr.Textbox(label = \"Bert Polarity\")\n",
    "            svm_sentiment = gr.Textbox(label = \"SVM Subjectivity\")\n",
    "            svm_polarity = gr.Textbox(label = \"SVM Polarity\")\n",
    "            \n",
    "    with gr.Row():       \n",
    "        #Displays scrapped tweets if option is selected\n",
    "        scraped_tweets = gr.DataFrame(label = \"Scraped Data\", headers=['Cleaned Text',\n",
    "                                               'Bert Subjectivity',\n",
    "                                               'Bert Polarity',\n",
    "                                               'SVM Subjectivity',\n",
    "                                               'SVM Polarity'], wrap=True)\n",
    "    #Button to run nlp function\n",
    "    submit_button.click(nlp, \n",
    "                        inputs=[game_title,scrap_no,activate_scrape,algorithm_choice[0]], \n",
    "                        outputs=[bert_sentiment,\n",
    "                                 bert_polarity,\n",
    "                                 svm_sentiment,\n",
    "                                 svm_polarity,\n",
    "                                 scraped_tweets]\n",
    "                       )\n",
    "\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a406a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8fdfc141",
   "metadata": {},
   "source": [
    "# Performance Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50c670b",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_testing_tweets =  scraping_game(\"2022-01-01\", \"2022-11-04\", 'pubg', 2000)\n",
    "cleaned_testing_tweets = [pre_process(text) for text in raw_testing_tweets]\n",
    "cleaned_testing_tweets = [text for text in cleaned_testing_tweets if text and len(text.split())>1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177ec70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "length = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba206edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = []\n",
    "bert_time = []\n",
    "svm_time =[]\n",
    "for i in range(100,2000,20):\n",
    "    print(f\"========== {i} tweets ==========\")\n",
    "    txt = cleaned_testing_tweets[:i]\n",
    "    result_bert_sub, result_bert_polar, bert_exe_time = bert_predict(txt) # return individual results\n",
    "    result_svm_sub, result_svm_polar, svm_exe_time = svm_predict(txt)\n",
    "    x.append(i)\n",
    "    bert_time.append(bert_exe_time)\n",
    "    svm_time.append(svm_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4990bf33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# print original performance\n",
    "plt.plot(x,bert_time, label='bert execution time (in seconds)')\n",
    "plt.xlabel = 'number of tweets'\n",
    "plt.ylabel = 'execution time (seconds)'\n",
    "plt.legend()\n",
    "\n",
    "poly = np.polyfit(x, bert_time, deg=1)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(x, bert_time, label='bert execution time (in seconds)')\n",
    "ax.plot(x, np.polyval(poly, x), label='fitted data')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a455c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print original performance\n",
    "plt.plot(x,svm_time, label='svm execution time (in seconds)')\n",
    "plt.xlabel = 'number of tweets'\n",
    "plt.ylabel = 'execution time (seconds)'\n",
    "plt.legend()\n",
    "\n",
    "poly = np.polyfit(x, svm_time, deg=1)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(x, svm_time, label='svm execution time (in seconds)')\n",
    "ax.plot(x, np.polyval(poly, x), label='fitted data')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d146ee93",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "2d40b353eadc8cbb987c2ca5c7db5aac5278416995cd628ee8f7b8b865cb97db"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
